<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Clustering &mdash; PyPruning  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Mixed Quadratic Programming" href="MIQP.html" />
    <link rel="prev" title="Ranking" href="rank.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> PyPruning
            <img src="_static/pruning-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">PyPruning</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="available.html">Pruning an ensemble</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="greedy.html">Greedy</a></li>
<li class="toctree-l2"><a class="reference internal" href="rank.html">Ranking</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="MIQP.html">Mixed Quadratic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="prox.html">Proximal Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="random.html">Random</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending PyPruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="papers.html">Reproducing results from literature</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyPruning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="available.html">Pruning an ensemble</a> &raquo;</li>
      <li>Clustering</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/sbuschjaeger/pypruning/blob/master/docs/cluster.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-PyPruning.ClusterPruningClassifier">
<span id="clustering"></span><h1>Clustering<a class="headerlink" href="#module-PyPruning.ClusterPruningClassifier" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.ClusterPruningClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">PyPruning.ClusterPruningClassifier.</span></span><span class="sig-name descname"><span class="pre">ClusterPruningClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">n_estimators=5</span></em>, <em class="sig-param"><span class="pre">cluster_estimators=&lt;function</span> <span class="pre">kmeans&gt;</span></em>, <em class="sig-param"><span class="pre">select_estimators=&lt;function</span> <span class="pre">random_selector&gt;</span></em>, <em class="sig-param"><span class="pre">cluster_mode='probabilities'</span></em>, <em class="sig-param"><span class="pre">cluster_options=None</span></em>, <em class="sig-param"><span class="pre">selector_options=None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.ClusterPruningClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="extending.html#PyPruning.PruningClassifier.PruningClassifier" title="PyPruning.PruningClassifier.PruningClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyPruning.PruningClassifier.PruningClassifier</span></code></a></p>
<p>Clustering-based pruning.</p>
<p>Clustering-based methods follow a two-step procedure. In first step, they cluster the estimators in the ensemble according to some clustering algorithm. Then, in the second, a representative form each cluster is selected to form the pruned ensemble. More formally, clustering-based pruning uses the following optimization problem:</p>
<p>In this implementation, you must provide two functions</p>
<ul class="simple">
<li><p><cite>cluster_estimators</cite>: A function which clusters the estimators given their representation X (see <cite>cluster_mode</cite> for details) and return the cluster assignment for each estimator. An example of kmeans clustering would be:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kmeans</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">n_estimators</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">assignments</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">assignments</span>
</pre></div>
</div>
<ul class="simple">
<li><p><cite>select_estimators</cite>: A function which selects the estimators from the clustering and returns the selected indices. An example of which selects the centroids would be:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">centroid_selector</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">assignments</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

    <span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">assignments</span><span class="p">)</span>
    <span class="n">centroids</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">centroids_</span>

    <span class="n">centroid_idx</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">pairwise_distances_argmin_min</span><span class="p">(</span><span class="n">centroids</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">centroid_idx</span>
</pre></div>
</div>
<p>If you want to pass additional parameter to <cite>cluster_estimators</cite> or <cite>select_estimators</cite> you can do so via the <cite>cluster_options</cite> and <cite>selector_options</cite> respectively. These parameters are passed via <a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs to the functions so please make sure that they are either <cite>None</cite> or valid Python dictionaries.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.n_estimators">
<span class="sig-name descname"><span class="pre">n_estimators</span></span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.n_estimators" title="Permalink to this definition"></a></dt>
<dd><p>The number of estimators which should be selected.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int, default is 5</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.cluster_estimators">
<span class="sig-name descname"><span class="pre">cluster_estimators</span></span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.cluster_estimators" title="Permalink to this definition"></a></dt>
<dd><p>A function that clusters the classifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>function, default is kmeans</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.select_estimators">
<span class="sig-name descname"><span class="pre">select_estimators</span></span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.select_estimators" title="Permalink to this definition"></a></dt>
<dd><p>A function that selects representatives from each cluster</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>function, default is random_selector</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.cluster_mode">
<span class="sig-name descname"><span class="pre">cluster_mode</span></span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.cluster_mode" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>The representation of each estimator used for clustering. Must be one of {“probabilities”, “predictions”, “accuracy”}:</dt><dd><ul class="simple">
<li><p>“probabilities”: Uses the raw probability output of each estimator for clustering. For multi-class problems the vector is “flattened” to a N * C vector where N is the number pf data points in the pruning set and C is the number of classes</p></li>
<li><p>“predictions”: Same as “probabilities”, but uses the predictions instead of the probabilities.</p></li>
<li><p>“accuracy”: Computes the accuracy of each estimator on each datapoint and uses the corresponding vector for clustering.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str, default is probabilities”</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.cluster_options">
<span class="sig-name descname"><span class="pre">cluster_options</span></span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.cluster_options" title="Permalink to this definition"></a></dt>
<dd><p>Additional options passed to <cite>cluster_estimators</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict, default is None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.selector_options">
<span class="sig-name descname"><span class="pre">selector_options</span></span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.selector_options" title="Permalink to this definition"></a></dt>
<dd><p>Additional options passed to <cite>select_estimators</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict, default is None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.prune_">
<span class="sig-name descname"><span class="pre">prune_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">proba</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.ClusterPruningClassifier.prune_" title="Permalink to this definition"></a></dt>
<dd><p>Prunes the ensemble using the ensemble predictions proba and the pruning data targets / data. If the pruning method requires access to the original ensemble members you can access these via <a href="#id3"><span class="problematic" id="id4">self.estimators_</span></a>. Note that <a href="#id5"><span class="problematic" id="id6">self.estimators_</span></a> is already a deep-copy of the estimators so you are also free to change the estimators in this list if you want to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>proba</strong> (<em>numpy matrix</em>) – A (N,M,C) matrix which contains the individual predictions of each ensemble member on the pruning data. Each ensemble prediction is generated via predict_proba. N is size of the pruning data, M the size of the base ensemble and C is the number of classes</p></li>
<li><p><strong>target</strong> (<em>numpy array of ints</em>) – A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,…,C-1 are used</p></li>
<li><p><strong>data</strong> (<em>numpy matrix</em><em>, </em><em>optional</em>) – The data points in a (N, M) matrix on which the proba has been computed, where N is the pruning set size and M is the number of classifier in the original ensemble. This can be used by a pruning method if required, but most methods do not require the actual data points but only the individual predictions.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>A tuple of indices and weights (idx, weights) with the following properties</em></p></li>
<li><p><strong>idx</strong> (<em>numpy array / list of ints</em>) – A list of integers which classifier should be selected from <a href="#id7"><span class="problematic" id="id8">self.estimators_</span></a>. Any changes made to <a href="#id9"><span class="problematic" id="id10">self.estimators_</span></a> are also reflected here, so make sure that the order of classifier in proba and <a href="#id11"><span class="problematic" id="id12">self.estimators_</span></a> remains the same (or you return idx accordingly)</p></li>
<li><p><strong>weights</strong> (<em>numpy array / list of floats</em>) – The individual weights for each selected classifier. The size of this array should match the size of idx (and not the size of the original base ensemble).</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.agglomerative">
<span class="sig-prename descclassname"><span class="pre">PyPruning.ClusterPruningClassifier.</span></span><span class="sig-name descname"><span class="pre">agglomerative</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.agglomerative" title="Permalink to this definition"></a></dt>
<dd><p>Perform agglomerative clustering on the given data <cite>X</cite>. The original publication (see below) considers the accuracy / error of each estimator which can be achieved by setting <cite>cluster_mode = “accuracy”</cite> in the ClusterPruningClassifier.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Giacinto, G., Roli, F., &amp; Fumera, G. (n.d.). Design of effective multiple classifier systems by clustering of classifiers. Proceedings 15th International Conference on Pattern Recognition. ICPR-2000. doi:10.1109/icpr.2000.906039</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.centroid_selector">
<span class="sig-prename descclassname"><span class="pre">PyPruning.ClusterPruningClassifier.</span></span><span class="sig-name descname"><span class="pre">centroid_selector</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assignments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.centroid_selector" title="Permalink to this definition"></a></dt>
<dd><p>Returns the centroid of each cluster. Bakker and Heske propose this approach, although there are subtle differences. Originally they propose to use annealing via an EM algorithm, whereas we use kmeans / agglomerative clustering.</p>
<dl class="simple">
<dt>Reference</dt><dd><p>Bakker, Bart, and Tom Heskes. “Clustering ensembles of neural network models.” Neural networks 16.2 (2003): 261-269.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.cluster_accuracy">
<span class="sig-prename descclassname"><span class="pre">PyPruning.ClusterPruningClassifier.</span></span><span class="sig-name descname"><span class="pre">cluster_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assignments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.cluster_accuracy" title="Permalink to this definition"></a></dt>
<dd><p>Select the most accurate model from each cluster. Lazarevic and Obradovic propose this approach although there are subtle differences. In the original paper they remove the least-accurate classifier as long as the performance of the sub-ensemble does not decrease. In this implementation we simply select the best / most accurate classifier from each cluster.</p>
<dl class="simple">
<dt>Reference</dt><dd><p>Lazarevic, A., &amp; Obradovic, Z. (2001). Effective pruning of neural network classifier ensembles. Proceedings of the International Joint Conference on Neural Networks, 2(January), 796–801. <a class="reference external" href="https://doi.org/10.1109/ijcnn.2001.939461">https://doi.org/10.1109/ijcnn.2001.939461</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.kmeans">
<span class="sig-prename descclassname"><span class="pre">PyPruning.ClusterPruningClassifier.</span></span><span class="sig-name descname"><span class="pre">kmeans</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.kmeans" title="Permalink to this definition"></a></dt>
<dd><p>Perform kmeans clustering on given data <cite>X</cite>. The original publication (see below) considers the predictions of each estimator which can be achieved by setting <cite>cluster_mode = “predictions”</cite> in the ClusterPruningClassifier. Second, the original publication discusses binary classification problems. In this multi-class implementation the proba/predictions for each class are flattened before clustering.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Lazarevic, A., &amp; Obradovic, Z. (2001). Effective pruning of neural network classifier ensembles. Proceedings of the International Joint Conference on Neural Networks, 2(January), 796–801. <a class="reference external" href="https://doi.org/10.1109/ijcnn.2001.939461pdf">https://doi.org/10.1109/ijcnn.2001.939461pdf</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.largest_mean_distance">
<span class="sig-prename descclassname"><span class="pre">PyPruning.ClusterPruningClassifier.</span></span><span class="sig-name descname"><span class="pre">largest_mean_distance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assignments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.largest_mean_distance" title="Permalink to this definition"></a></dt>
<dd><p>Select the most distant classifier to all other clusters.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p>Giacinto, G., Roli, F., &amp; Fumera, G. (n.d.). Design of effective multiple classifier systems by clustering of classifiers. Proceedings 15th International Conference on Pattern Recognition. ICPR-2000. doi:10.1109/icpr.2000.906039</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="PyPruning.ClusterPruningClassifier.random_selector">
<span class="sig-prename descclassname"><span class="pre">PyPruning.ClusterPruningClassifier.</span></span><span class="sig-name descname"><span class="pre">random_selector</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assignments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#PyPruning.ClusterPruningClassifier.random_selector" title="Permalink to this definition"></a></dt>
<dd><p>Randomly select a classifier from each cluster.</p>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="rank.html" class="btn btn-neutral float-left" title="Ranking" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="MIQP.html" class="btn btn-neutral float-right" title="Mixed Quadratic Programming" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Sebastian Buschjäger.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>