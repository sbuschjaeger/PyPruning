<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>PyPruning.RankPruningClassifier API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PyPruning.RankPruningClassifier</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np

from sklearn.metrics import roc_auc_score, cohen_kappa_score
from sklearn.metrics.pairwise import cosine_similarity

from joblib import Parallel,delayed

from .PruningClassifier import PruningClassifier

def individual_margin_diversity(i, ensemble_proba, target, alpha = 0.2):
    &#39;&#39;&#39;
    Computes the individual diversity of the classifier wrt. to the ensemble and its contribution to the margin. alpha controls the trade-off between both values.

    Note: The paper uses alpha = 0.2 in all experiments and reports that it worked well. Thus, it is also the default value here. If you want to change this value you can use `partial` to set it to a different value (e.g. 0.5) before creating a new RankPruningClassifier:

    ```Python
        from functools import partial
        m_function = partial(individual_margin_diversity, alpha = 0.5)
        pruner = RankPruningClassifier(n_estimators = 10, metric = m_function, n_jobs = 8)
    ```

    Reference:
        Guo, H., Liu, H., Li, R., Wu, C., Guo, Y., &amp; Xu, M. (2018). Margin &amp; diversity based ordering ensemble pruning. Neurocomputing, 275, 237–246. https://doi.org/10.1016/j.neucom.2017.06.052
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:]
    n = iproba.shape[0]

    predictions = iproba.argmax(axis=1)
    V = np.zeros(ensemble_proba.shape)
    idx = ensemble_proba.argmax(axis=2)
    V[np.arange(ensemble_proba.shape[0])[:,None],np.arange(ensemble_proba.shape[1]),idx] = 1
    V = V.sum(axis=0)

    #V = jproba #all_proba.sum(axis=0)#.argmax(axis=1)
    MDM = 0
    
    for j in range(n):
        if (predictions[j] == target[j]):
            
            # special case for margin: prediction for label with majority of votes
            if(predictions[j] == np.argmax(V[j,:])):
                # calculate margin with second highest number of votes
                sortedArray = np.sort(np.copy(V[j,:]))
                
                # check whether 1. and 2. max vot counts are equal! (margin = 0)
                if(sortedArray[-2] == np.max(V[j,:])):
                    margin = (  V[j, target[j]]  - (sortedArray[-2] -1)   ) / n
                else:
                    margin = (  V[j, target[j]]  - sortedArray[-2]   ) / n
                   
            else:
                # usual case for margin: prediction not label with majority of votes
                margin = (  V[j, target[j]]  - np.max(V[j,:])   ) / n
            
            
            # somehow theres still a rare case for margin == 0
            if(margin == 0):
                margin = 0.01
            
            fm = np.log(abs(margin))
            fd = np.log(V[j, target[j]] / n)
            MDM = MDM + (alpha*fm) + ((1-alpha)*fd)
    return - 1.0 * MDM

def individual_contribution(i, ensemble_proba, target):
    &#39;&#39;&#39;
    Compute the individual contributions of each classifier wrt. the entire ensemble. Return the negative contribution due to the minimization.

    Reference:
        Lu, Z., Wu, X., Zhu, X., &amp; Bongard, J. (2010). Ensemble pruning via individual contribution ordering. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 871–880. https://doi.org/10.1145/1835804.1835914
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:]
    n = iproba.shape[0]

    predictions = iproba.argmax(axis=1)
    V = np.zeros(ensemble_proba.shape)
    idx = ensemble_proba.argmax(axis=2)
    V[np.arange(ensemble_proba.shape[0])[:,None],np.arange(ensemble_proba.shape[1]),idx] = 1
    V = V.sum(axis=0)

    IC = 0
    #V = all_proba.argmax(axis=2)
    #predictions = iproba.argmax(axis=1)
    #V = all_proba.sum(axis=0)#.argmax(axis=1)

    for j in range(n):
        if (predictions[j] == target[j]):
            
            # case 1 (minority group)
            # label with majority votes on datapoint  = np.argmax(V[j, :]) 
            if(predictions[j] != np.argmax(V[j,:])):
                IC = IC + (2*(np.max(V[j,:])) - V[j, predictions[j]])
                
            else: # case 2 (majority group)
                # calculate second largest nr of votes on datapoint i
                sortedArray = np.sort(np.copy(V[j,:]))
                IC = IC + (sortedArray[-2])
                
        else:
            # case 3 (wrong prediction)
            IC = IC + (V[j, target[j]]  -  V[j, predictions[j]] - np.max(V[j,:]) )
    return - 1.0 * IC

def individual_error(i, ensemble_proba, target):
    &#39;&#39;&#39; 
    Compute the error for the individual classifier.
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:]
    return (iproba.argmax(axis=1) != target).mean()

def individual_neg_auc(i, ensemble_proba, target):
    &#39;&#39;&#39; 
    Compute the roc auc score for the individual classifier, but return its negative value for minimization.
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:]
    if(iproba.shape[1] == 2):
        iproba = iproba.argmax(axis=1)
        return - 1.0 * roc_auc_score(target, iproba)
    else:
        return - 1.0 * roc_auc_score(target, iproba, multi_class=&#34;ovr&#34;)

def individual_kappa_statistic(i, ensemble_proba, target):
    &#39;&#39;&#39; 
    Compute the Cohen-Kappa statistic for the individual classifier with respect to the entire ensemble.

    Reference:
        Margineantu, D., &amp; Dietterich, T. G. (1997). Pruning Adaptive Boosting. Proceedings of the Fourteenth International Conference on Machine Learning, 211–218. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.7017&amp;rep=rep1&amp;type=pdf
    &#39;&#39;&#39;
    scores = []
    iproba = ensemble_proba[i,:,:].argmax(axis=1)

    for j, jproba in enumerate(ensemble_proba):
        if j != i:
            # See https://github.com/scikit-learn/scikit-learn/issues/14256
            # and https://stackoverflow.com/questions/14861891/runtimewarning-invalid-value-encountered-in-divide
            with np.errstate(divide=&#39;ignore&#39;,invalid=&#39;ignore&#39;):
                score = cohen_kappa_score(iproba, jproba.argmax(axis=1))
                if np.isnan(score):
                    scores.append(0.0)
                else:
                    scores.append(score)
    return min(scores)

def reference_vector(i, ensemble_proba, target):
    &#39;&#39;&#39;
    Compare how close the individual predictions is to the entire ensemble&#39;s prediction by using the cosine_similary

    Note: The paper describes a slightly different distance metric compared to what is implemented here. The paper uses a projection to a reference vector, but - unfortunately - does not explain the specific implementation in detail. However, the authors also note two things:
    
    - (1) They use all classifier with an angle &lt;= pi/2 which can lead to more than n_estimator classifier. Thus we need to present an ordering based on the angles and pick the first n_estimator.
    - (2) &#34;The classifiers are ordered by increasing values of the angle between the signature vectors of the individual classifiers and the reference vector&#34;. 
    
    `ref` and `ipred` (see source code) follow the exact definitions as presented in the paper (eq. 3) and the cosine_similary is the most direct implementation of &#34;the angle between signature and reference vector&#34; 


    Reference:
        Hernández-Lobato, D., Martínez-Muñoz, G., &amp; Suárez, A. (2006). Pruning in Ordered Bagging Ensembles. International Conference on Machine Learning, 1266–1273. https://doi.org/10.1109/ijcnn.2006.246837
    &#39;&#39;&#39;
    ref = 2 * (ensemble_proba.argmax(axis=1) == target) - 1.0
    ipred = 2 * (ensemble_proba[i,:].argmax(axis=1) == target) - 1.0
    return -1.0 * cosine_similarity(ipred, ref)

class RankPruningClassifier(PruningClassifier):
    &#39;&#39;&#39; Rank pruning. 
    
    Ranking methods assign a rank to each classifier in the ensemble and then select the best n_estimators according to this ranking. To rate each classifier a metric must be given. A metric is a function with receives three parameters:
        
    - `i` (int): The classifier which should be rated
    - `ensemble_proba` (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes
    - `target` (list / array): A list / array of class targets.

    A simple example for this function would be the individual error of each method:
    
    ```Python
        def individual_error(i, ensemble_proba, target):
            iproba = ensemble_proba[i,:,:]
            return (iproba.argmax(axis=1) != target).mean()
    ```

    **Important** The classifiers are sorted in ascending order and the first n_estimators are selected. Differently put, the metric is always minimized.

    Attributes
    ----------
    n_estimators : int, default is 5
        The number of estimators which should be selected.
    metric : function, default is individual_error 
        A function that assigns a score to each classifier which is then used for sorting
    n_jobs : int, default is 8
        The number of threads used for computing the individual metrics for each classifier.
    &#39;&#39;&#39;
    def __init__(self, 
        n_estimators = 5, 
        metric = individual_error,
        n_jobs = 8):

        super().__init__()

        assert metric is not None, &#34;You must provide a valid metric!&#34;
        self.n_estimators = n_estimators
        self.n_jobs = n_jobs
        self.metric = metric

    def prune_(self, proba, target, data = None):
        n_received = len(proba)
        if self.n_estimators &gt;= n_received:
            return range(0, n_received), [1.0 / n_received for _ in range(n_received)]
        
        single_scores = Parallel(n_jobs=self.n_jobs, backend=&#34;threading&#34;)(
            delayed(self.metric) (i, proba, target) for i in range(n_received)
        )
        single_scores = np.array(single_scores)

        return np.argpartition(single_scores, self.n_estimators)[:self.n_estimators], [1.0 / self.n_estimators for _ in range(self.n_estimators)]
        </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="PyPruning.RankPruningClassifier.individual_contribution"><code class="name flex">
<span>def <span class="ident">individual_contribution</span></span>(<span>i, ensemble_proba, target)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the individual contributions of each classifier wrt. the entire ensemble. Return the negative contribution due to the minimization.</p>
<h2 id="reference">Reference</h2>
<p>Lu, Z., Wu, X., Zhu, X., &amp; Bongard, J. (2010). Ensemble pruning via individual contribution ordering. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 871–880. <a href="https://doi.org/10.1145/1835804.1835914">https://doi.org/10.1145/1835804.1835914</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def individual_contribution(i, ensemble_proba, target):
    &#39;&#39;&#39;
    Compute the individual contributions of each classifier wrt. the entire ensemble. Return the negative contribution due to the minimization.

    Reference:
        Lu, Z., Wu, X., Zhu, X., &amp; Bongard, J. (2010). Ensemble pruning via individual contribution ordering. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 871–880. https://doi.org/10.1145/1835804.1835914
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:]
    n = iproba.shape[0]

    predictions = iproba.argmax(axis=1)
    V = np.zeros(ensemble_proba.shape)
    idx = ensemble_proba.argmax(axis=2)
    V[np.arange(ensemble_proba.shape[0])[:,None],np.arange(ensemble_proba.shape[1]),idx] = 1
    V = V.sum(axis=0)

    IC = 0
    #V = all_proba.argmax(axis=2)
    #predictions = iproba.argmax(axis=1)
    #V = all_proba.sum(axis=0)#.argmax(axis=1)

    for j in range(n):
        if (predictions[j] == target[j]):
            
            # case 1 (minority group)
            # label with majority votes on datapoint  = np.argmax(V[j, :]) 
            if(predictions[j] != np.argmax(V[j,:])):
                IC = IC + (2*(np.max(V[j,:])) - V[j, predictions[j]])
                
            else: # case 2 (majority group)
                # calculate second largest nr of votes on datapoint i
                sortedArray = np.sort(np.copy(V[j,:]))
                IC = IC + (sortedArray[-2])
                
        else:
            # case 3 (wrong prediction)
            IC = IC + (V[j, target[j]]  -  V[j, predictions[j]] - np.max(V[j,:]) )
    return - 1.0 * IC</code></pre>
</details>
</dd>
<dt id="PyPruning.RankPruningClassifier.individual_error"><code class="name flex">
<span>def <span class="ident">individual_error</span></span>(<span>i, ensemble_proba, target)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the error for the individual classifier.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def individual_error(i, ensemble_proba, target):
    &#39;&#39;&#39; 
    Compute the error for the individual classifier.
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:]
    return (iproba.argmax(axis=1) != target).mean()</code></pre>
</details>
</dd>
<dt id="PyPruning.RankPruningClassifier.individual_kappa_statistic"><code class="name flex">
<span>def <span class="ident">individual_kappa_statistic</span></span>(<span>i, ensemble_proba, target)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Cohen-Kappa statistic for the individual classifier with respect to the entire ensemble.</p>
<h2 id="reference">Reference</h2>
<p>Margineantu, D., &amp; Dietterich, T. G. (1997). Pruning Adaptive Boosting. Proceedings of the Fourteenth International Conference on Machine Learning, 211–218. <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.7017&amp;rep=rep1&amp;type=pdf">https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.7017&amp;rep=rep1&amp;type=pdf</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def individual_kappa_statistic(i, ensemble_proba, target):
    &#39;&#39;&#39; 
    Compute the Cohen-Kappa statistic for the individual classifier with respect to the entire ensemble.

    Reference:
        Margineantu, D., &amp; Dietterich, T. G. (1997). Pruning Adaptive Boosting. Proceedings of the Fourteenth International Conference on Machine Learning, 211–218. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.7017&amp;rep=rep1&amp;type=pdf
    &#39;&#39;&#39;
    scores = []
    iproba = ensemble_proba[i,:,:].argmax(axis=1)

    for j, jproba in enumerate(ensemble_proba):
        if j != i:
            # See https://github.com/scikit-learn/scikit-learn/issues/14256
            # and https://stackoverflow.com/questions/14861891/runtimewarning-invalid-value-encountered-in-divide
            with np.errstate(divide=&#39;ignore&#39;,invalid=&#39;ignore&#39;):
                score = cohen_kappa_score(iproba, jproba.argmax(axis=1))
                if np.isnan(score):
                    scores.append(0.0)
                else:
                    scores.append(score)
    return min(scores)</code></pre>
</details>
</dd>
<dt id="PyPruning.RankPruningClassifier.individual_margin_diversity"><code class="name flex">
<span>def <span class="ident">individual_margin_diversity</span></span>(<span>i, ensemble_proba, target, alpha=0.2)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the individual diversity of the classifier wrt. to the ensemble and its contribution to the margin. alpha controls the trade-off between both values.</p>
<p>Note: The paper uses alpha = 0.2 in all experiments and reports that it worked well. Thus, it is also the default value here. If you want to change this value you can use <code>partial</code> to set it to a different value (e.g. 0.5) before creating a new RankPruningClassifier:</p>
<pre><code class="language-Python">    from functools import partial
    m_function = partial(individual_margin_diversity, alpha = 0.5)
    pruner = RankPruningClassifier(n_estimators = 10, metric = m_function, n_jobs = 8)
</code></pre>
<h2 id="reference">Reference</h2>
<p>Guo, H., Liu, H., Li, R., Wu, C., Guo, Y., &amp; Xu, M. (2018). Margin &amp; diversity based ordering ensemble pruning. Neurocomputing, 275, 237–246. <a href="https://doi.org/10.1016/j.neucom.2017.06.052">https://doi.org/10.1016/j.neucom.2017.06.052</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def individual_margin_diversity(i, ensemble_proba, target, alpha = 0.2):
    &#39;&#39;&#39;
    Computes the individual diversity of the classifier wrt. to the ensemble and its contribution to the margin. alpha controls the trade-off between both values.

    Note: The paper uses alpha = 0.2 in all experiments and reports that it worked well. Thus, it is also the default value here. If you want to change this value you can use `partial` to set it to a different value (e.g. 0.5) before creating a new RankPruningClassifier:

    ```Python
        from functools import partial
        m_function = partial(individual_margin_diversity, alpha = 0.5)
        pruner = RankPruningClassifier(n_estimators = 10, metric = m_function, n_jobs = 8)
    ```

    Reference:
        Guo, H., Liu, H., Li, R., Wu, C., Guo, Y., &amp; Xu, M. (2018). Margin &amp; diversity based ordering ensemble pruning. Neurocomputing, 275, 237–246. https://doi.org/10.1016/j.neucom.2017.06.052
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:]
    n = iproba.shape[0]

    predictions = iproba.argmax(axis=1)
    V = np.zeros(ensemble_proba.shape)
    idx = ensemble_proba.argmax(axis=2)
    V[np.arange(ensemble_proba.shape[0])[:,None],np.arange(ensemble_proba.shape[1]),idx] = 1
    V = V.sum(axis=0)

    #V = jproba #all_proba.sum(axis=0)#.argmax(axis=1)
    MDM = 0
    
    for j in range(n):
        if (predictions[j] == target[j]):
            
            # special case for margin: prediction for label with majority of votes
            if(predictions[j] == np.argmax(V[j,:])):
                # calculate margin with second highest number of votes
                sortedArray = np.sort(np.copy(V[j,:]))
                
                # check whether 1. and 2. max vot counts are equal! (margin = 0)
                if(sortedArray[-2] == np.max(V[j,:])):
                    margin = (  V[j, target[j]]  - (sortedArray[-2] -1)   ) / n
                else:
                    margin = (  V[j, target[j]]  - sortedArray[-2]   ) / n
                   
            else:
                # usual case for margin: prediction not label with majority of votes
                margin = (  V[j, target[j]]  - np.max(V[j,:])   ) / n
            
            
            # somehow theres still a rare case for margin == 0
            if(margin == 0):
                margin = 0.01
            
            fm = np.log(abs(margin))
            fd = np.log(V[j, target[j]] / n)
            MDM = MDM + (alpha*fm) + ((1-alpha)*fd)
    return - 1.0 * MDM</code></pre>
</details>
</dd>
<dt id="PyPruning.RankPruningClassifier.individual_neg_auc"><code class="name flex">
<span>def <span class="ident">individual_neg_auc</span></span>(<span>i, ensemble_proba, target)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the roc auc score for the individual classifier, but return its negative value for minimization.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def individual_neg_auc(i, ensemble_proba, target):
    &#39;&#39;&#39; 
    Compute the roc auc score for the individual classifier, but return its negative value for minimization.
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:]
    if(iproba.shape[1] == 2):
        iproba = iproba.argmax(axis=1)
        return - 1.0 * roc_auc_score(target, iproba)
    else:
        return - 1.0 * roc_auc_score(target, iproba, multi_class=&#34;ovr&#34;)</code></pre>
</details>
</dd>
<dt id="PyPruning.RankPruningClassifier.reference_vector"><code class="name flex">
<span>def <span class="ident">reference_vector</span></span>(<span>i, ensemble_proba, target)</span>
</code></dt>
<dd>
<div class="desc"><p>Compare how close the individual predictions is to the entire ensemble's prediction by using the cosine_similary</p>
<p>Note: The paper describes a slightly different distance metric compared to what is implemented here. The paper uses a projection to a reference vector, but - unfortunately - does not explain the specific implementation in detail. However, the authors also note two things:</p>
<ul>
<li>(1) They use all classifier with an angle &lt;= pi/2 which can lead to more than n_estimator classifier. Thus we need to present an ordering based on the angles and pick the first n_estimator.</li>
<li>(2) "The classifiers are ordered by increasing values of the angle between the signature vectors of the individual classifiers and the reference vector". </li>
</ul>
<p><code>ref</code> and <code>ipred</code> (see source code) follow the exact definitions as presented in the paper (eq. 3) and the cosine_similary is the most direct implementation of "the angle between signature and reference vector" </p>
<h2 id="reference">Reference</h2>
<p>Hernández-Lobato, D., Martínez-Muñoz, G., &amp; Suárez, A. (2006). Pruning in Ordered Bagging Ensembles. International Conference on Machine Learning, 1266–1273. <a href="https://doi.org/10.1109/ijcnn.2006.246837">https://doi.org/10.1109/ijcnn.2006.246837</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reference_vector(i, ensemble_proba, target):
    &#39;&#39;&#39;
    Compare how close the individual predictions is to the entire ensemble&#39;s prediction by using the cosine_similary

    Note: The paper describes a slightly different distance metric compared to what is implemented here. The paper uses a projection to a reference vector, but - unfortunately - does not explain the specific implementation in detail. However, the authors also note two things:
    
    - (1) They use all classifier with an angle &lt;= pi/2 which can lead to more than n_estimator classifier. Thus we need to present an ordering based on the angles and pick the first n_estimator.
    - (2) &#34;The classifiers are ordered by increasing values of the angle between the signature vectors of the individual classifiers and the reference vector&#34;. 
    
    `ref` and `ipred` (see source code) follow the exact definitions as presented in the paper (eq. 3) and the cosine_similary is the most direct implementation of &#34;the angle between signature and reference vector&#34; 


    Reference:
        Hernández-Lobato, D., Martínez-Muñoz, G., &amp; Suárez, A. (2006). Pruning in Ordered Bagging Ensembles. International Conference on Machine Learning, 1266–1273. https://doi.org/10.1109/ijcnn.2006.246837
    &#39;&#39;&#39;
    ref = 2 * (ensemble_proba.argmax(axis=1) == target) - 1.0
    ipred = 2 * (ensemble_proba[i,:].argmax(axis=1) == target) - 1.0
    return -1.0 * cosine_similarity(ipred, ref)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PyPruning.RankPruningClassifier.RankPruningClassifier"><code class="flex name class">
<span>class <span class="ident">RankPruningClassifier</span></span>
<span>(</span><span>n_estimators=5, metric=&lt;function individual_error&gt;, n_jobs=8)</span>
</code></dt>
<dd>
<div class="desc"><p>Rank pruning. </p>
<p>Ranking methods assign a rank to each classifier in the ensemble and then select the best n_estimators according to this ranking. To rate each classifier a metric must be given. A metric is a function with receives three parameters:</p>
<ul>
<li><code>i</code> (int): The classifier which should be rated</li>
<li><code>ensemble_proba</code> (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes</li>
<li><code>target</code> (list / array): A list / array of class targets.</li>
</ul>
<p>A simple example for this function would be the individual error of each method:</p>
<pre><code class="language-Python">    def individual_error(i, ensemble_proba, target):
        iproba = ensemble_proba[i,:,:]
        return (iproba.argmax(axis=1) != target).mean()
</code></pre>
<p><strong>Important</strong> The classifiers are sorted in ascending order and the first n_estimators are selected. Differently put, the metric is always minimized.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>n_estimators</code></strong> :&ensp;<code>int</code>, default <code>is 5</code></dt>
<dd>The number of estimators which should be selected.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>function</code>, default <code>is <a title="PyPruning.RankPruningClassifier.individual_error" href="#PyPruning.RankPruningClassifier.individual_error">individual_error()</a> </code></dt>
<dd>A function that assigns a score to each classifier which is then used for sorting</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, default <code>is 8</code></dt>
<dd>The number of threads used for computing the individual metrics for each classifier.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RankPruningClassifier(PruningClassifier):
    &#39;&#39;&#39; Rank pruning. 
    
    Ranking methods assign a rank to each classifier in the ensemble and then select the best n_estimators according to this ranking. To rate each classifier a metric must be given. A metric is a function with receives three parameters:
        
    - `i` (int): The classifier which should be rated
    - `ensemble_proba` (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes
    - `target` (list / array): A list / array of class targets.

    A simple example for this function would be the individual error of each method:
    
    ```Python
        def individual_error(i, ensemble_proba, target):
            iproba = ensemble_proba[i,:,:]
            return (iproba.argmax(axis=1) != target).mean()
    ```

    **Important** The classifiers are sorted in ascending order and the first n_estimators are selected. Differently put, the metric is always minimized.

    Attributes
    ----------
    n_estimators : int, default is 5
        The number of estimators which should be selected.
    metric : function, default is individual_error 
        A function that assigns a score to each classifier which is then used for sorting
    n_jobs : int, default is 8
        The number of threads used for computing the individual metrics for each classifier.
    &#39;&#39;&#39;
    def __init__(self, 
        n_estimators = 5, 
        metric = individual_error,
        n_jobs = 8):

        super().__init__()

        assert metric is not None, &#34;You must provide a valid metric!&#34;
        self.n_estimators = n_estimators
        self.n_jobs = n_jobs
        self.metric = metric

    def prune_(self, proba, target, data = None):
        n_received = len(proba)
        if self.n_estimators &gt;= n_received:
            return range(0, n_received), [1.0 / n_received for _ in range(n_received)]
        
        single_scores = Parallel(n_jobs=self.n_jobs, backend=&#34;threading&#34;)(
            delayed(self.metric) (i, proba, target) for i in range(n_received)
        )
        single_scores = np.array(single_scores)

        return np.argpartition(single_scores, self.n_estimators)[:self.n_estimators], [1.0 / self.n_estimators for _ in range(self.n_estimators)]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PyPruning.PruningClassifier.PruningClassifier" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier">PruningClassifier</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PyPruning.PruningClassifier.PruningClassifier" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier">PruningClassifier</a></b></code>:
<ul class="hlist">
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.predict" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.predict">predict</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.predict_proba" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.predict_proba">predict_proba</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.prune" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.prune">prune</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.prune_" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.prune_">prune_</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<img src="../../images/ls8.png" alt="LS8">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="PyPruning" href="index.html">PyPruning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="PyPruning.RankPruningClassifier.individual_contribution" href="#PyPruning.RankPruningClassifier.individual_contribution">individual_contribution</a></code></li>
<li><code><a title="PyPruning.RankPruningClassifier.individual_error" href="#PyPruning.RankPruningClassifier.individual_error">individual_error</a></code></li>
<li><code><a title="PyPruning.RankPruningClassifier.individual_kappa_statistic" href="#PyPruning.RankPruningClassifier.individual_kappa_statistic">individual_kappa_statistic</a></code></li>
<li><code><a title="PyPruning.RankPruningClassifier.individual_margin_diversity" href="#PyPruning.RankPruningClassifier.individual_margin_diversity">individual_margin_diversity</a></code></li>
<li><code><a title="PyPruning.RankPruningClassifier.individual_neg_auc" href="#PyPruning.RankPruningClassifier.individual_neg_auc">individual_neg_auc</a></code></li>
<li><code><a title="PyPruning.RankPruningClassifier.reference_vector" href="#PyPruning.RankPruningClassifier.reference_vector">reference_vector</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PyPruning.RankPruningClassifier.RankPruningClassifier" href="#PyPruning.RankPruningClassifier.RankPruningClassifier">RankPruningClassifier</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
The software is written and maintained by <a href="https://sbuschjaeger.github.io/">Sebastian Buschjäger</a> as part of his work at the <a href="https://www-ai.cs.tu-dortmund.de">Chair for Artificial Intelligence</a> at the TU Dortmund University and the <a href="https://sfb876.tu-dortmund.de">Collaborative Research Center 876, project A1</a>. If you have any question feel free to contact me under <a href="mailto:sebstian.buschjaeger@tu-dortmund.de">sebstian.buschjaeger@tu-dortmund.de</a>.
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>