<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>PyPruning.PruningClassifier API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PyPruning.PruningClassifier</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from abc import ABC, abstractmethod 
import copy

import numpy as np

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier

from sklearn.base import BaseEstimator, ClassifierMixin

class PruningClassifier(ABC): 
    &#39;&#39;&#39; This abstract class forms the basis of all pruning methods and offers a unified interface. New pruning methods must extend this class and implement the prune_ method as detailed below. 

    Attributes
    ----------
    weights_: numpy array 
        An array of weights corresponding to each classifier in self.estimators_
    estimators_ : list
        A list of estimators
    n_classes_ : int 
        The number of classes the pruned ensemble supports.
    &#39;&#39;&#39;
    def __init__(self):
        self.weights_ = None
        self.estimators_ = None
        self.n_classes_ = None


    @abstractmethod
    def prune_(self, proba, target, data = None):
        &#39;&#39;&#39;
        Prunes the ensemble using the ensemble predictions proba and the pruning data targets / data. If the pruning method requires access to the original ensemble members you can access these via self.estimators_. Note that self.estimators_ is already a deep-copy of the estimators so you are also free to change the estimators in this list if you want to.

        Parameters
        ----------
        proba : numpy matrix
            A (N,M,C) matrix which contains the individual predictions of each ensemble member on the pruning data. Each ensemble prediction is generated via predict_proba. N is size of the pruning data, M the size of the base ensemble and C is the number of classes
        
        target: numpy array of ints 
            A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,...,C-1 are used
        
        data:  numpy matrix, optional
            The data points in a (N, M) matrix on which the proba has been computed, where N is the pruning set size and M is the number of classifier in the original ensemble. This can be used by a pruning method if required, but most methods do not require the actual data points but only the individual predictions. 
        
        Returns
        -------
        A tuple of indices and weights (idx, weights) with the following properties:
        idx : numpy array / list of ints
            A list of integers which classifier should be selected from self.estimators_. Any changes made to self.estimators_ are also reflected here, so make sure that the order of classifier in proba and self.estimators_ remains the same (or you return idx accordingly)
        
        weights: numpy array / list of floats
            The individual weights for each selected classifier. The size of this array should match the size of idx (and not the size of the original base ensemble). 
        &#39;&#39;&#39;
        pass
    
    def prune(self, X, y, estimators, classes = None, n_classes = None):
        &#39;&#39;&#39;
        Prunes the given ensemble on the supplied dataset. There are a few assumptions placed on the behavior of the individual classifiers in `estimators`. If you use scikit-learn classifier and any classifier implementing their interface they should work without a problem. The detailed assumptions are listed below:
         
        - `predict_proba`: Each estimator should offer a predict_proba function which returns the class probabilities for each class on a batch of data
        - `n_classes_`: Each estimator should offer a field on the number of classes it has been trained on. Ideally, this should be the same for all classifier in the ensemble but might differ e.g. due to different bootstrap samples. This field is not accessed if you manually supply `n_classes` as parameter to this function
        - `classes_`: Each estimator should offer a class mapping which shows the order of classes returned by predict_proba. Usually this should simply be [0,1,2,3,4] for 5 classes, but if your classifier returns class probabilities in a different order, e.g. [2,1,0,3,4] you should store this order in `classes_`. This field is not accessed if you manually supply `classes` as parameter to this function

        For pruning this function calls `predict_proba` on each classifier in `estimators` and then calls `prune_` of the implementing class. After pruning, it extracts the selected classifiers from `estimators` with their corresponding weight and stores them in `self.weights_` and `self.estimators_`

        Parameters
        ----------
        X : numpy matrix
            A (N, d) matrix with the datapoints used for pruning where N is the number of data points and d is the dimensionality
        
        Y : numpy array / list of ints
            A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,...,C-1 are used
        
        estimators : list
            A list of estimators from which the pruned ensemble is selected.
        
        classes : numpy array / list of ints
            Contains the class mappings of each base learner in the order which is returned by predict_proba. Usually this should be something like [0,1,2,3,4] for a 5 class problem. However, sometimes weird stuff happens and the mapping might be [2,1,0,3,4]. In this case, you can manually supply the list of mappings
        
        n_classes: int
            The total number of classes. Usually, this it should be n_classes = len(classes). However, sometimes estimators are only fitted on a subset of data (e.g. during cross validation or bootstrapping) and the prune set might contain classes which are not in the original training set and vice-versa. In this case its best to supply n_classes beforehand. 

        Returns
        -------
        The pruned ensemble.
        &#39;&#39;&#39;
        if classes is None:
            classes = [e.n_classes_ for e in estimators]
            if (len(set(classes)) &gt; 1):
                raise RuntimeError(&#34;Detected a different number of classes for each learner. Please make sure that all learners have their n_classes_ field set to the same value. Alternatively, you may supply a list of classes via the classes parameter to avoid this error.&#34;)
                #self.n_classes_ = max(classes)
            else:
                self.classes_ = estimators[0].classes_
                self.n_classes_ = classes[0]
            
            if len(set(y)) &gt; self.n_classes_:
                raise RuntimeError(&#34;Detected more classes in the pruning set then the estimators were originally trained on. This usually results in errors or unpredicted classification errors. You can supply a list of classes via the classes parameter. Classes should be arrays / lists containing all possible class labels starting from 0 to C, where C is the number of classes. Please make sure that these are integers as they will be interpreted as such.&#34;)
        else:
            self.classes_ = classes
            self.n_classes_ = n_classes

        # Okay this is a bit crazy, but has its reasons. This basically implements the for-loop below, but also takes care of the case where a single estimator did not receive all the labels. In this case predict_proba returns vectors with less than n_classes entries. This can happen in ExtraTrees, but also in RF, especially with unfavorable cross validation splits or large class imbalances. 
        # Anyway, this code construct the desired matrix and copies all predictions to the corresponding locations based on e.classes_. This **should** be correct for numeric classes staring by 0 and also anything which is mapped via the SKLearns LabelEncoder.  
        proba = np.zeros(shape=(len(estimators), X.shape[0], self.n_classes_), dtype=np.float32)
        for i, e in enumerate(estimators):
            proba[i, :, self.classes_.astype(int)] = e.predict_proba(X).T

        # proba = []
        # for h in estimators:
        #     proba.append(h.predict_proba(X))
        # proba = np.array(proba)

        self.estimators_ = copy.deepcopy(estimators)
        idx, weights = self.prune_(proba, y, X)        
        estimators_ = []
        for i in idx:
            estimators_.append(self.estimators_[i])
        
        self.estimators_ = estimators_
        self.weights_ = weights 
        
        return self

    def _individual_proba(self, X):
        &#39;&#39;&#39; Predict class probabilities for each individual learner in the ensemble without considering the weights.

        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The samples to be predicted.

        Returns
        -------
        y : array, shape (n_samples,C)
            The predicted class probabilities for each learner.
        &#39;&#39;&#39;
        assert self.estimators_ is not None, &#34;Call prune before calling predict_proba!&#34;
        all_proba = []

        for e in self.estimators_:
            tmp = np.zeros(shape=(X.shape[0], self.n_classes_), dtype=np.float32)
            tmp[:, self.classes_.astype(int)] += e.predict_proba(X)
            all_proba.append(tmp)

        if len(all_proba) == 0:
            return np.zeros(shape=(1, X.shape[0], self.n_classes_), dtype=np.float32)
        else:
            return np.array(all_proba)

    def predict_proba(self, X):
        &#39;&#39;&#39; Predict class probabilities using the pruned model.

        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The samples to be predicted.

        Returns
        -------
        y : array, shape (n_samples,C)
            The predicted class probabilities. 
        &#39;&#39;&#39;
        all_proba = self._individual_proba(X)
        scaled_prob = np.array([w * p for w,p in zip(all_proba, self.weights_)])
        combined_proba = np.sum(scaled_prob, axis=0)
        return combined_proba

    def predict(self, X):
        &#39;&#39;&#39; Predict classes using the pruned model.

        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The samples to be predicted.

        Returns
        -------
        y : array, shape (n_samples,)
            The predicted classes. 

        &#39;&#39;&#39;
        proba = self.predict_proba(X)
        return self.classes_.take(proba.argmax(axis=1), axis=0)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PyPruning.PruningClassifier.PruningClassifier"><code class="flex name class">
<span>class <span class="ident">PruningClassifier</span></span>
</code></dt>
<dd>
<div class="desc"><p>This abstract class forms the basis of all pruning methods and offers a unified interface. New pruning methods must extend this class and implement the prune_ method as detailed below. </p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>weights_</code></strong> :&ensp;<code>numpy array </code></dt>
<dd>An array of weights corresponding to each classifier in self.estimators_</dd>
<dt><strong><code>estimators_</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of estimators</dd>
<dt><strong><code>n_classes_</code></strong> :&ensp;<code>int </code></dt>
<dd>The number of classes the pruned ensemble supports.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PruningClassifier(ABC): 
    &#39;&#39;&#39; This abstract class forms the basis of all pruning methods and offers a unified interface. New pruning methods must extend this class and implement the prune_ method as detailed below. 

    Attributes
    ----------
    weights_: numpy array 
        An array of weights corresponding to each classifier in self.estimators_
    estimators_ : list
        A list of estimators
    n_classes_ : int 
        The number of classes the pruned ensemble supports.
    &#39;&#39;&#39;
    def __init__(self):
        self.weights_ = None
        self.estimators_ = None
        self.n_classes_ = None


    @abstractmethod
    def prune_(self, proba, target, data = None):
        &#39;&#39;&#39;
        Prunes the ensemble using the ensemble predictions proba and the pruning data targets / data. If the pruning method requires access to the original ensemble members you can access these via self.estimators_. Note that self.estimators_ is already a deep-copy of the estimators so you are also free to change the estimators in this list if you want to.

        Parameters
        ----------
        proba : numpy matrix
            A (N,M,C) matrix which contains the individual predictions of each ensemble member on the pruning data. Each ensemble prediction is generated via predict_proba. N is size of the pruning data, M the size of the base ensemble and C is the number of classes
        
        target: numpy array of ints 
            A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,...,C-1 are used
        
        data:  numpy matrix, optional
            The data points in a (N, M) matrix on which the proba has been computed, where N is the pruning set size and M is the number of classifier in the original ensemble. This can be used by a pruning method if required, but most methods do not require the actual data points but only the individual predictions. 
        
        Returns
        -------
        A tuple of indices and weights (idx, weights) with the following properties:
        idx : numpy array / list of ints
            A list of integers which classifier should be selected from self.estimators_. Any changes made to self.estimators_ are also reflected here, so make sure that the order of classifier in proba and self.estimators_ remains the same (or you return idx accordingly)
        
        weights: numpy array / list of floats
            The individual weights for each selected classifier. The size of this array should match the size of idx (and not the size of the original base ensemble). 
        &#39;&#39;&#39;
        pass
    
    def prune(self, X, y, estimators, classes = None, n_classes = None):
        &#39;&#39;&#39;
        Prunes the given ensemble on the supplied dataset. There are a few assumptions placed on the behavior of the individual classifiers in `estimators`. If you use scikit-learn classifier and any classifier implementing their interface they should work without a problem. The detailed assumptions are listed below:
         
        - `predict_proba`: Each estimator should offer a predict_proba function which returns the class probabilities for each class on a batch of data
        - `n_classes_`: Each estimator should offer a field on the number of classes it has been trained on. Ideally, this should be the same for all classifier in the ensemble but might differ e.g. due to different bootstrap samples. This field is not accessed if you manually supply `n_classes` as parameter to this function
        - `classes_`: Each estimator should offer a class mapping which shows the order of classes returned by predict_proba. Usually this should simply be [0,1,2,3,4] for 5 classes, but if your classifier returns class probabilities in a different order, e.g. [2,1,0,3,4] you should store this order in `classes_`. This field is not accessed if you manually supply `classes` as parameter to this function

        For pruning this function calls `predict_proba` on each classifier in `estimators` and then calls `prune_` of the implementing class. After pruning, it extracts the selected classifiers from `estimators` with their corresponding weight and stores them in `self.weights_` and `self.estimators_`

        Parameters
        ----------
        X : numpy matrix
            A (N, d) matrix with the datapoints used for pruning where N is the number of data points and d is the dimensionality
        
        Y : numpy array / list of ints
            A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,...,C-1 are used
        
        estimators : list
            A list of estimators from which the pruned ensemble is selected.
        
        classes : numpy array / list of ints
            Contains the class mappings of each base learner in the order which is returned by predict_proba. Usually this should be something like [0,1,2,3,4] for a 5 class problem. However, sometimes weird stuff happens and the mapping might be [2,1,0,3,4]. In this case, you can manually supply the list of mappings
        
        n_classes: int
            The total number of classes. Usually, this it should be n_classes = len(classes). However, sometimes estimators are only fitted on a subset of data (e.g. during cross validation or bootstrapping) and the prune set might contain classes which are not in the original training set and vice-versa. In this case its best to supply n_classes beforehand. 

        Returns
        -------
        The pruned ensemble.
        &#39;&#39;&#39;
        if classes is None:
            classes = [e.n_classes_ for e in estimators]
            if (len(set(classes)) &gt; 1):
                raise RuntimeError(&#34;Detected a different number of classes for each learner. Please make sure that all learners have their n_classes_ field set to the same value. Alternatively, you may supply a list of classes via the classes parameter to avoid this error.&#34;)
                #self.n_classes_ = max(classes)
            else:
                self.classes_ = estimators[0].classes_
                self.n_classes_ = classes[0]
            
            if len(set(y)) &gt; self.n_classes_:
                raise RuntimeError(&#34;Detected more classes in the pruning set then the estimators were originally trained on. This usually results in errors or unpredicted classification errors. You can supply a list of classes via the classes parameter. Classes should be arrays / lists containing all possible class labels starting from 0 to C, where C is the number of classes. Please make sure that these are integers as they will be interpreted as such.&#34;)
        else:
            self.classes_ = classes
            self.n_classes_ = n_classes

        # Okay this is a bit crazy, but has its reasons. This basically implements the for-loop below, but also takes care of the case where a single estimator did not receive all the labels. In this case predict_proba returns vectors with less than n_classes entries. This can happen in ExtraTrees, but also in RF, especially with unfavorable cross validation splits or large class imbalances. 
        # Anyway, this code construct the desired matrix and copies all predictions to the corresponding locations based on e.classes_. This **should** be correct for numeric classes staring by 0 and also anything which is mapped via the SKLearns LabelEncoder.  
        proba = np.zeros(shape=(len(estimators), X.shape[0], self.n_classes_), dtype=np.float32)
        for i, e in enumerate(estimators):
            proba[i, :, self.classes_.astype(int)] = e.predict_proba(X).T

        # proba = []
        # for h in estimators:
        #     proba.append(h.predict_proba(X))
        # proba = np.array(proba)

        self.estimators_ = copy.deepcopy(estimators)
        idx, weights = self.prune_(proba, y, X)        
        estimators_ = []
        for i in idx:
            estimators_.append(self.estimators_[i])
        
        self.estimators_ = estimators_
        self.weights_ = weights 
        
        return self

    def _individual_proba(self, X):
        &#39;&#39;&#39; Predict class probabilities for each individual learner in the ensemble without considering the weights.

        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The samples to be predicted.

        Returns
        -------
        y : array, shape (n_samples,C)
            The predicted class probabilities for each learner.
        &#39;&#39;&#39;
        assert self.estimators_ is not None, &#34;Call prune before calling predict_proba!&#34;
        all_proba = []

        for e in self.estimators_:
            tmp = np.zeros(shape=(X.shape[0], self.n_classes_), dtype=np.float32)
            tmp[:, self.classes_.astype(int)] += e.predict_proba(X)
            all_proba.append(tmp)

        if len(all_proba) == 0:
            return np.zeros(shape=(1, X.shape[0], self.n_classes_), dtype=np.float32)
        else:
            return np.array(all_proba)

    def predict_proba(self, X):
        &#39;&#39;&#39; Predict class probabilities using the pruned model.

        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The samples to be predicted.

        Returns
        -------
        y : array, shape (n_samples,C)
            The predicted class probabilities. 
        &#39;&#39;&#39;
        all_proba = self._individual_proba(X)
        scaled_prob = np.array([w * p for w,p in zip(all_proba, self.weights_)])
        combined_proba = np.sum(scaled_prob, axis=0)
        return combined_proba

    def predict(self, X):
        &#39;&#39;&#39; Predict classes using the pruned model.

        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The samples to be predicted.

        Returns
        -------
        y : array, shape (n_samples,)
            The predicted classes. 

        &#39;&#39;&#39;
        proba = self.predict_proba(X)
        return self.classes_.take(proba.argmax(axis=1), axis=0)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="PyPruning.GreedyPruningClassifier.GreedyPruningClassifier" href="GreedyPruningClassifier.html#PyPruning.GreedyPruningClassifier.GreedyPruningClassifier">GreedyPruningClassifier</a></li>
<li><a title="PyPruning.MIQPPruningClassifier.MIQPPruningClassifier" href="MIQPPruningClassifier.html#PyPruning.MIQPPruningClassifier.MIQPPruningClassifier">MIQPPruningClassifier</a></li>
<li><a title="PyPruning.ProxPruningClassifier.ProxPruningClassifier" href="ProxPruningClassifier.html#PyPruning.ProxPruningClassifier.ProxPruningClassifier">ProxPruningClassifier</a></li>
<li><a title="PyPruning.RandomPruningClassifier.RandomPruningClassifier" href="RandomPruningClassifier.html#PyPruning.RandomPruningClassifier.RandomPruningClassifier">RandomPruningClassifier</a></li>
<li><a title="PyPruning.RankPruningClassifier.RankPruningClassifier" href="RankPruningClassifier.html#PyPruning.RankPruningClassifier.RankPruningClassifier">RankPruningClassifier</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyPruning.PruningClassifier.PruningClassifier.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict classes using the pruned model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> or <code>sparse matrix, shape (n_samples, n_features)</code></dt>
<dd>The samples to be predicted.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>array, shape (n_samples,)</code></dt>
<dd>The predicted classes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#39;&#39;&#39; Predict classes using the pruned model.

    Parameters
    ----------
    X : array-like or sparse matrix, shape (n_samples, n_features)
        The samples to be predicted.

    Returns
    -------
    y : array, shape (n_samples,)
        The predicted classes. 

    &#39;&#39;&#39;
    proba = self.predict_proba(X)
    return self.classes_.take(proba.argmax(axis=1), axis=0)</code></pre>
</details>
</dd>
<dt id="PyPruning.PruningClassifier.PruningClassifier.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict class probabilities using the pruned model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> or <code>sparse matrix, shape (n_samples, n_features)</code></dt>
<dd>The samples to be predicted.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>array, shape (n_samples,C)</code></dt>
<dd>The predicted class probabilities.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X):
    &#39;&#39;&#39; Predict class probabilities using the pruned model.

    Parameters
    ----------
    X : array-like or sparse matrix, shape (n_samples, n_features)
        The samples to be predicted.

    Returns
    -------
    y : array, shape (n_samples,C)
        The predicted class probabilities. 
    &#39;&#39;&#39;
    all_proba = self._individual_proba(X)
    scaled_prob = np.array([w * p for w,p in zip(all_proba, self.weights_)])
    combined_proba = np.sum(scaled_prob, axis=0)
    return combined_proba</code></pre>
</details>
</dd>
<dt id="PyPruning.PruningClassifier.PruningClassifier.prune"><code class="name flex">
<span>def <span class="ident">prune</span></span>(<span>self, X, y, estimators, classes=None, n_classes=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Prunes the given ensemble on the supplied dataset. There are a few assumptions placed on the behavior of the individual classifiers in <code>estimators</code>. If you use scikit-learn classifier and any classifier implementing their interface they should work without a problem. The detailed assumptions are listed below:</p>
<ul>
<li><code>predict_proba</code>: Each estimator should offer a predict_proba function which returns the class probabilities for each class on a batch of data</li>
<li><code>n_classes_</code>: Each estimator should offer a field on the number of classes it has been trained on. Ideally, this should be the same for all classifier in the ensemble but might differ e.g. due to different bootstrap samples. This field is not accessed if you manually supply <code>n_classes</code> as parameter to this function</li>
<li><code>classes_</code>: Each estimator should offer a class mapping which shows the order of classes returned by predict_proba. Usually this should simply be [0,1,2,3,4] for 5 classes, but if your classifier returns class probabilities in a different order, e.g. [2,1,0,3,4] you should store this order in <code>classes_</code>. This field is not accessed if you manually supply <code>classes</code> as parameter to this function</li>
</ul>
<p>For pruning this function calls <code>predict_proba</code> on each classifier in <code>estimators</code> and then calls <code>prune_</code> of the implementing class. After pruning, it extracts the selected classifiers from <code>estimators</code> with their corresponding weight and stores them in <code>self.weights_</code> and <code>self.estimators_</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy matrix</code></dt>
<dd>A (N, d) matrix with the datapoints used for pruning where N is the number of data points and d is the dimensionality</dd>
<dt><strong><code>Y</code></strong> :&ensp;<code>numpy array / list</code> of <code>ints</code></dt>
<dd>A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,&hellip;,C-1 are used</dd>
<dt><strong><code>estimators</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of estimators from which the pruned ensemble is selected.</dd>
<dt><strong><code>classes</code></strong> :&ensp;<code>numpy array / list</code> of <code>ints</code></dt>
<dd>Contains the class mappings of each base learner in the order which is returned by predict_proba. Usually this should be something like [0,1,2,3,4] for a 5 class problem. However, sometimes weird stuff happens and the mapping might be [2,1,0,3,4]. In this case, you can manually supply the list of mappings</dd>
<dt><strong><code>n_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>The total number of classes. Usually, this it should be n_classes = len(classes). However, sometimes estimators are only fitted on a subset of data (e.g. during cross validation or bootstrapping) and the prune set might contain classes which are not in the original training set and vice-versa. In this case its best to supply n_classes beforehand.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The pruned ensemble.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prune(self, X, y, estimators, classes = None, n_classes = None):
    &#39;&#39;&#39;
    Prunes the given ensemble on the supplied dataset. There are a few assumptions placed on the behavior of the individual classifiers in `estimators`. If you use scikit-learn classifier and any classifier implementing their interface they should work without a problem. The detailed assumptions are listed below:
     
    - `predict_proba`: Each estimator should offer a predict_proba function which returns the class probabilities for each class on a batch of data
    - `n_classes_`: Each estimator should offer a field on the number of classes it has been trained on. Ideally, this should be the same for all classifier in the ensemble but might differ e.g. due to different bootstrap samples. This field is not accessed if you manually supply `n_classes` as parameter to this function
    - `classes_`: Each estimator should offer a class mapping which shows the order of classes returned by predict_proba. Usually this should simply be [0,1,2,3,4] for 5 classes, but if your classifier returns class probabilities in a different order, e.g. [2,1,0,3,4] you should store this order in `classes_`. This field is not accessed if you manually supply `classes` as parameter to this function

    For pruning this function calls `predict_proba` on each classifier in `estimators` and then calls `prune_` of the implementing class. After pruning, it extracts the selected classifiers from `estimators` with their corresponding weight and stores them in `self.weights_` and `self.estimators_`

    Parameters
    ----------
    X : numpy matrix
        A (N, d) matrix with the datapoints used for pruning where N is the number of data points and d is the dimensionality
    
    Y : numpy array / list of ints
        A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,...,C-1 are used
    
    estimators : list
        A list of estimators from which the pruned ensemble is selected.
    
    classes : numpy array / list of ints
        Contains the class mappings of each base learner in the order which is returned by predict_proba. Usually this should be something like [0,1,2,3,4] for a 5 class problem. However, sometimes weird stuff happens and the mapping might be [2,1,0,3,4]. In this case, you can manually supply the list of mappings
    
    n_classes: int
        The total number of classes. Usually, this it should be n_classes = len(classes). However, sometimes estimators are only fitted on a subset of data (e.g. during cross validation or bootstrapping) and the prune set might contain classes which are not in the original training set and vice-versa. In this case its best to supply n_classes beforehand. 

    Returns
    -------
    The pruned ensemble.
    &#39;&#39;&#39;
    if classes is None:
        classes = [e.n_classes_ for e in estimators]
        if (len(set(classes)) &gt; 1):
            raise RuntimeError(&#34;Detected a different number of classes for each learner. Please make sure that all learners have their n_classes_ field set to the same value. Alternatively, you may supply a list of classes via the classes parameter to avoid this error.&#34;)
            #self.n_classes_ = max(classes)
        else:
            self.classes_ = estimators[0].classes_
            self.n_classes_ = classes[0]
        
        if len(set(y)) &gt; self.n_classes_:
            raise RuntimeError(&#34;Detected more classes in the pruning set then the estimators were originally trained on. This usually results in errors or unpredicted classification errors. You can supply a list of classes via the classes parameter. Classes should be arrays / lists containing all possible class labels starting from 0 to C, where C is the number of classes. Please make sure that these are integers as they will be interpreted as such.&#34;)
    else:
        self.classes_ = classes
        self.n_classes_ = n_classes

    # Okay this is a bit crazy, but has its reasons. This basically implements the for-loop below, but also takes care of the case where a single estimator did not receive all the labels. In this case predict_proba returns vectors with less than n_classes entries. This can happen in ExtraTrees, but also in RF, especially with unfavorable cross validation splits or large class imbalances. 
    # Anyway, this code construct the desired matrix and copies all predictions to the corresponding locations based on e.classes_. This **should** be correct for numeric classes staring by 0 and also anything which is mapped via the SKLearns LabelEncoder.  
    proba = np.zeros(shape=(len(estimators), X.shape[0], self.n_classes_), dtype=np.float32)
    for i, e in enumerate(estimators):
        proba[i, :, self.classes_.astype(int)] = e.predict_proba(X).T

    # proba = []
    # for h in estimators:
    #     proba.append(h.predict_proba(X))
    # proba = np.array(proba)

    self.estimators_ = copy.deepcopy(estimators)
    idx, weights = self.prune_(proba, y, X)        
    estimators_ = []
    for i in idx:
        estimators_.append(self.estimators_[i])
    
    self.estimators_ = estimators_
    self.weights_ = weights 
    
    return self</code></pre>
</details>
</dd>
<dt id="PyPruning.PruningClassifier.PruningClassifier.prune_"><code class="name flex">
<span>def <span class="ident">prune_</span></span>(<span>self, proba, target, data=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Prunes the ensemble using the ensemble predictions proba and the pruning data targets / data. If the pruning method requires access to the original ensemble members you can access these via self.estimators_. Note that self.estimators_ is already a deep-copy of the estimators so you are also free to change the estimators in this list if you want to.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>proba</code></strong> :&ensp;<code>numpy matrix</code></dt>
<dd>A (N,M,C) matrix which contains the individual predictions of each ensemble member on the pruning data. Each ensemble prediction is generated via predict_proba. N is size of the pruning data, M the size of the base ensemble and C is the number of classes</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>numpy array</code> of <code>ints </code></dt>
<dd>A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,&hellip;,C-1 are used</dd>
<dt><strong><code>data</code></strong> :&ensp;<code> numpy matrix</code>, optional</dt>
<dd>The data points in a (N, M) matrix on which the proba has been computed, where N is the pruning set size and M is the number of classifier in the original ensemble. This can be used by a pruning method if required, but most methods do not require the actual data points but only the individual predictions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>A tuple</code> of <code>indices and weights (idx, weights) with the following properties:</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>idx</code></strong> :&ensp;<code>numpy array / list</code> of <code>ints</code></dt>
<dd>A list of integers which classifier should be selected from self.estimators_. Any changes made to self.estimators_ are also reflected here, so make sure that the order of classifier in proba and self.estimators_ remains the same (or you return idx accordingly)</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>numpy array / list</code> of <code>floats</code></dt>
<dd>The individual weights for each selected classifier. The size of this array should match the size of idx (and not the size of the original base ensemble).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def prune_(self, proba, target, data = None):
    &#39;&#39;&#39;
    Prunes the ensemble using the ensemble predictions proba and the pruning data targets / data. If the pruning method requires access to the original ensemble members you can access these via self.estimators_. Note that self.estimators_ is already a deep-copy of the estimators so you are also free to change the estimators in this list if you want to.

    Parameters
    ----------
    proba : numpy matrix
        A (N,M,C) matrix which contains the individual predictions of each ensemble member on the pruning data. Each ensemble prediction is generated via predict_proba. N is size of the pruning data, M the size of the base ensemble and C is the number of classes
    
    target: numpy array of ints 
        A numpy array or list of N integers where each integer represents the class for each example. Classes should start with 0, so that for C classes the integer 0,1,...,C-1 are used
    
    data:  numpy matrix, optional
        The data points in a (N, M) matrix on which the proba has been computed, where N is the pruning set size and M is the number of classifier in the original ensemble. This can be used by a pruning method if required, but most methods do not require the actual data points but only the individual predictions. 
    
    Returns
    -------
    A tuple of indices and weights (idx, weights) with the following properties:
    idx : numpy array / list of ints
        A list of integers which classifier should be selected from self.estimators_. Any changes made to self.estimators_ are also reflected here, so make sure that the order of classifier in proba and self.estimators_ remains the same (or you return idx accordingly)
    
    weights: numpy array / list of floats
        The individual weights for each selected classifier. The size of this array should match the size of idx (and not the size of the original base ensemble). 
    &#39;&#39;&#39;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<img src="../../images/ls8.png" alt="LS8">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="PyPruning" href="index.html">PyPruning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PyPruning.PruningClassifier.PruningClassifier" href="#PyPruning.PruningClassifier.PruningClassifier">PruningClassifier</a></code></h4>
<ul class="">
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.predict" href="#PyPruning.PruningClassifier.PruningClassifier.predict">predict</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.predict_proba" href="#PyPruning.PruningClassifier.PruningClassifier.predict_proba">predict_proba</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.prune" href="#PyPruning.PruningClassifier.PruningClassifier.prune">prune</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.prune_" href="#PyPruning.PruningClassifier.PruningClassifier.prune_">prune_</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
The software is written and maintained by <a href="https://sbuschjaeger.github.io/">Sebastian Buschj√§ger</a> as part of his work at the <a href="https://www-ai.cs.tu-dortmund.de">Chair for Artificial Intelligence</a> at the TU Dortmund University and the <a href="https://sfb876.tu-dortmund.de">Collaborative Research Center 876, project A1</a>. If you have any question feel free to contact me under <a href="mailto:sebstian.buschjaeger@tu-dortmund.de">sebstian.buschjaeger@tu-dortmund.de</a>.
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>