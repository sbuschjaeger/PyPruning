<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>PyPruning.ProxPruningClassifier API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PyPruning.ProxPruningClassifier</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from joblib import Parallel, delayed
import numbers
import time
from sklearn import ensemble
from tqdm import tqdm
import os
from sklearn.metrics import accuracy_score

from scipy.special import softmax
from sklearn.tree import DecisionTreeClassifier

from .PruningClassifier import PruningClassifier

# Modified from https://stackoverflow.com/questions/38157972/how-to-implement-mini-batch-gradient-descent-in-python
def create_mini_batches(inputs, targets, data, batch_size, shuffle=False):
    assert inputs.shape[0] == targets.shape[0]
    indices = np.arange(inputs.shape[0])
    if shuffle:
        np.random.shuffle(indices)
    
    start_idx = 0
    while start_idx &lt; len(indices):
        if start_idx + batch_size &gt; len(indices) - 1:
            excerpt = indices[start_idx:]
        else:
            excerpt = indices[start_idx:start_idx + batch_size]
        
        start_idx += batch_size

        yield inputs[excerpt], targets[excerpt], data[excerpt]

# See https://eng.ucmerced.edu/people/wwang5/papers/SimplexProj.pdf for details
def to_prob_simplex(x):
    if x is None or len(x) == 0:
        return x
    sorted_x = np.sort(x)
    x_sum = sorted_x[0]
    l = 1.0 - sorted_x[0]
    for i in range(1,len(sorted_x)):
        x_sum += sorted_x[i]
        tmp = 1.0 / (i + 1.0) * (1.0 - x_sum)
        if (sorted_x[i] + tmp) &gt; 0:
            l = tmp 
    
    return [max(xi + l, 0.0) for xi in x]

class ProxPruningClassifier(PruningClassifier):
    &#34;&#34;&#34; (Heterogeneous) Pruning via Proximal Gradient Descent
    
    This pruning method directly minimizes
    $$
        \\arg\\min_w L \\left(\sum_{i=1}^M w_i h_i(x), y\\right) + \\lambda \\sum_{i=1}^K w_i R(h_i) 
    $$
    via (stochastic) proximal gradient descent. Currently the `{mse, cross-entropy, hinge2}` are supported. In addition to the loss functions two types of regularizer can be chosen:

    - `ensemble_regularizer`: This regularizer tries to remove as many members as possible from the ensemble as possible. If you want to select exactly K elements you can choose the `hard-L0` constraint. Otherwise &#34;soft variations&#34; of this in the form of `L0` and `L1` regularization are also available.
    - `tree_regularizer`: This regularizer tries to choose smaller trees with fewer nodes over larger ones. This regularizer is basically the number of nodes present in a tree.

    Attributes
    ----------
    step_size : float
        The step_size used for stochastic gradient descent for opt 
    loss : str
        The loss function for training. Should be one of `{&#34;mse&#34;, &#34;cross-entropy&#34;, &#34;hinge2&#34;}`
    normalize_weights : boolean
        True if nonzero weights should be projected onto the probability simplex, that is they should sum to 1. 
    ensemble_regularizer : str
        The ensemble_regularizer. Should be one of `{None, &#34;L0&#34;, &#34;L1&#34;, &#34;hard-L1&#34;}`
    l_ensemble_reg : float
        The ensemble_regularizer regularization strength. 
    tree_regularizer : str
        The tree_regularizer. Should be one of `{None,&#34;node&#34;}`
    l_tree_reg : float
        The tree_regularizer regularization strength. 
    batch_size: int
        The batch sized used for SGD
    epochs : int
        The number of epochs SGD is run.
    verbose : boolean
        If true, shows a progress bar via tqdm and some statistics
    out_path: str
        If set, stores a file called epoch_$i.npy with the statistics for epoch $i under the given path.
    estimators_ : list of objects
        The list of estimators which are used to built the ensemble. Each estimator must offer a predict_proba method.
    weights_ : np.array of floats
        The list of weights corresponding to their respective estimator in self.estimators_. 
    &#34;&#34;&#34;

    def __init__(self,
        loss = &#34;cross-entropy&#34;,
        step_size = 1e-1,
        ensemble_regularizer = &#34;L1&#34;,
        l_ensemble_reg = 0,  
        tree_regularizer = &#34;node&#34;,
        l_tree_reg = 0,
        normalize_weights = True,
        batch_size = 256,
        epochs = 1,
        verbose = False, 
        update_leaves = False,
        out_path = None,
        eval_every_epochs = None):

        assert loss in [&#34;mse&#34;,&#34;cross-entropy&#34;,&#34;hinge2&#34;], &#34;Currently only {{mse, cross-entropy, hinge2}} loss is supported&#34;
        assert ensemble_regularizer is None or ensemble_regularizer in [&#34;none&#34;,&#34;L0&#34;, &#34;L1&#34;, &#34;hard-L1&#34;], &#34;Currently only {{none,L0, L1, hard-L1}} the ensemble regularizer is supported&#34;
        assert l_tree_reg &gt;= 0, &#34;l_reg must be greater or equal to 0&#34;
        assert tree_regularizer is None or tree_regularizer in [&#34;node&#34;], &#34;Currently only {{none, node}} regularizer is supported for tree the regularizer.&#34;
        assert batch_size &gt;= 1, &#34;batch_size must be at-least 1&#34;
        assert epochs &gt;= 1, &#34;epochs must be at-least 1&#34;

        if ensemble_regularizer == &#34;hard-L1&#34;:
            assert l_ensemble_reg &gt;= 1 or l_ensemble_reg == 0, &#34;You chose ensemble_regularizer = hard-L1, but set 0 &lt; l_ensemble_reg &lt; 1 which does not really makes sense. If hard-L1 is set, then l_ensemble_reg is the maximum number of estimators in the pruned ensemble, thus likely an integer value &gt;= 1.&#34;

        super().__init__()
        
        self.loss = loss
        self.step_size = step_size
        self.ensemble_regularizer = ensemble_regularizer
        self.l_ensemble_reg = l_ensemble_reg
        self.tree_regularizer = tree_regularizer
        self.l_tree_reg = l_tree_reg
        self.normalize_weights = normalize_weights
        self.batch_size = batch_size
        self.epochs = epochs
        self.verbose = verbose
        self.update_leaves = update_leaves
        self.out_path = out_path
        self.eval_every_epochs = eval_every_epochs

    def next(self, proba, target, data):
        # If we update the leaves, then proba also changes and we need to recompute them. Otherwise we can just use the pre-computed probas
        if self.update_leaves:
            proba = self._individual_proba(data)
        else:
            proba = np.swapaxes(proba, 0, 1)

        output = np.array([w * p for w,p in zip(proba, self.weights_)]).sum(axis=0)

        batch_size = output.shape[0]
        accuracy = (output.argmax(axis=1) == target) * 100.0
        n_trees = [self.num_trees() for _ in range(batch_size)]
        n_param = [self.num_parameters() for _ in range(batch_size)]
        
        # Compute the appropriate loss. 
        if self.loss == &#34;mse&#34;:
            target_one_hot = np.array( [ [1.0 if y == i else 0.0 for i in range(self.n_classes_)] for y in target] )
            loss = (output - target_one_hot) * (output - target_one_hot)
            loss_deriv = 2 * (output - target_one_hot)
        elif self.loss == &#34;cross-entropy&#34;:
            target_one_hot = np.array( [ [1.0 if y == i else 0.0 for i in range(self.n_classes_)] for y in target] )
            p = softmax(output, axis=1)
            loss = -target_one_hot*np.log(p + 1e-7)
            m = target.shape[0]
            loss_deriv = softmax(output, axis=1)
            loss_deriv[range(m),target_one_hot.argmax(axis=1)] -= 1
        elif self.loss == &#34;hinge2&#34;:
            target_one_hot = np.array( [ [1.0 if y == i else -1.0 for i in range(self.n_classes_)] for y in target] )
            zeros = np.zeros_like(target_one_hot)
            loss = np.maximum(1.0 - target_one_hot * output, zeros)**2
            loss_deriv = - 2 * target_one_hot * np.maximum(1.0 - target_one_hot * output, zeros) 
        else:
            raise &#34;Currently only the losses {{cross-entropy, mse, hinge2}} are supported, but you provided: {}&#34;.format(self.loss)
        
        loss = np.sum(np.mean(loss,axis=1))
        
        if self.ensemble_regularizer == &#34;L0&#34;:
            loss += self.l_ensemble_reg * np.linalg.norm(self.weights_,0)
        elif self.ensemble_regularizer == &#34;L1&#34;:
            loss += self.l_ensemble_reg * np.linalg.norm(self.weights_,1)

        # Compute the gradients for the loss
        directions = np.mean(proba*loss_deriv,axis=(1,2))

        # Compute the appropriate regularizer
        if self.tree_regularizer == &#34;node&#34; and self.l_tree_reg &gt; 0:
            loss += self.l_tree_reg * np.sum( [ (w * est.tree_.node_count) for w, est in zip(self.weights_, self.estimators_)] )
            
            node_deriv = self.l_tree_reg * np.array([ est.tree_.node_count for est in self.estimators_])
        else:
            node_deriv = 0

        # Perform the gradient step + projection 
        tmp_w = self.weights_ - self.step_size*directions - self.step_size*node_deriv
        
        if self.update_leaves:
            # compute direction per tree
            # tree_deriv = proba*loss_deriv
            for i, h in enumerate(self.estimators_):
                tree_grad = (self.weights_[i] * loss_deriv)[:,np.newaxis,:]
                # find idx
                idx = h.apply(data)
                h.tree_.value[idx] = h.tree_.value[idx] - self.step_size * tree_grad[:,:,h.classes_.astype(int)]
                # update model
                #h.tree_.value[idx] = h.tree_.value[idx] - self.step_size*h.tree_.value[idx]*tree_deriv[i,:,np.newaxis]
                
                #step = self.step_size*tree_deriv[i,:,np.newaxis]
                #h.tree_.value[idx] = h.tree_.value[idx] - step[:,:,self.classes_.astype(int)]

        if self.ensemble_regularizer == &#34;L0&#34;:
            tmp = np.sqrt(2 * self.l_ensemble_reg * self.step_size)
            tmp_w = np.array([0 if abs(w) &lt; tmp else w for w in tmp_w])
        elif self.ensemble_regularizer == &#34;L1&#34;:
            sign = np.sign(tmp_w)
            tmp_w = np.abs(tmp_w) - self.step_size*self.l_ensemble_reg
            tmp_w = sign*np.maximum(tmp_w,0)
        elif self.ensemble_regularizer == &#34;hard-L1&#34;:
            top_K = np.argsort(tmp_w)[-self.l_ensemble_reg:]
            tmp_w = np.array([w if i in top_K else 0 for i,w in enumerate(tmp_w)])

        # If set, normalize the weights. Note that we use the support of tmp_w for the projection onto the probability simplex
        # as described in http://proceedings.mlr.press/v28/kyrillidis13.pdf
        # Thus, we first need to extract the nonzero weights, project these and then copy them back into corresponding array
        
        if self.normalize_weights and len(tmp_w) &gt; 0:
            nonzero_idx = np.nonzero(tmp_w)[0]
            nonzero_w = tmp_w[nonzero_idx]
            nonzero_w = to_prob_simplex(nonzero_w)
            self.weights_ = np.zeros((len(tmp_w)))
            for i,w in zip(nonzero_idx, nonzero_w):
                self.weights_[i] = w
        else:
            self.weights_ = tmp_w
        
        return {&#34;loss&#34;:loss, &#34;accuracy&#34;: accuracy, &#34;num_trees&#34;: n_trees, &#34;num_parameters&#34; : n_param}

    def num_trees(self):
        return np.count_nonzero(self.weights_)

    def num_parameters(self):
        return sum( [ est.tree_.node_count if w != 0 else 0 for w, est in zip(self.weights_, self.estimators_)] )

    def prune_(self, proba, target, data):
        proba = np.swapaxes(proba, 0, 1)
        self.weights_ = np.array([1.0 / proba.shape[1] for _ in range(proba.shape[1])])

        if self.update_leaves:
            # SKlearn stores the raw counts instead of probabilities. For SGD its better to have the 
            # probabilities for numerical stability. 
            # tree.tree_.value is not writeable, but we can modify the values inplace. Thus we 
            # use [:] to copy the array into the normalized array. Also tree.tree_.value has a strange shape (batch_size, 1, n_classes)
            for tree in self.estimators_:
                tree.tree_.value[:] = tree.tree_.value / tree.tree_.value.sum(axis=(1,2))[:,np.newaxis,np.newaxis]

        for epoch in range(self.epochs):

            mini_batches = create_mini_batches(proba, target, data, self.batch_size, True) 

            times = []
            total_time = 0
            metrics = {}
            example_cnt = 0

            with tqdm(total=proba.shape[0], ncols=150, disable = not self.verbose) as pbar:
                for batch in mini_batches:
                    bproba, btarget, bdata = batch 

                    # Update Model                    
                    start_time = time.time()
                    batch_metrics = self.next(bproba, btarget, bdata)
                    batch_time = time.time() - start_time

                    # Extract statistics
                    for key,val in batch_metrics.items():
                        metrics[key] = np.concatenate( (metrics.get(key,[]), val), axis=None )
                        metrics[key + &#34;_sum&#34;] = metrics.get( key + &#34;_sum&#34;,0) + np.sum(val)

                    example_cnt += bproba.shape[0]
                    pbar.update(bproba.shape[0])
                    
                    # TODO ADD times to metrics and write it to disk
                    times.append(batch_time)
                    total_time += batch_time

                    m_str = &#34;&#34;
                    for key,val in metrics.items():
                        if &#34;_sum&#34; in key:
                            m_str += &#34;{} {:2.4f} &#34;.format(key.split(&#34;_sum&#34;)[0], val / example_cnt)
                    
                    desc = &#39;[{}/{}] {} time_item {:2.4f}&#39;.format(
                        epoch, 
                        self.epochs-1, 
                        m_str,
                        total_time / example_cnt
                    )
                    pbar.set_description(desc)
                
                if self.eval_every_epochs is not None and epoch % self.eval_every_epochs == 0 and self.out_path is not None:
                    np.save(os.path.join(self.out_path, &#34;epoch_{}.npy&#34;.format(epoch)), metrics, allow_pickle=True)
    
        return [i for i in range(len(self.weights_)) if self.weights_[i] &gt; 0], [w for w in self.weights_ if w &gt; 0]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="PyPruning.ProxPruningClassifier.create_mini_batches"><code class="name flex">
<span>def <span class="ident">create_mini_batches</span></span>(<span>inputs, targets, data, batch_size, shuffle=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_mini_batches(inputs, targets, data, batch_size, shuffle=False):
    assert inputs.shape[0] == targets.shape[0]
    indices = np.arange(inputs.shape[0])
    if shuffle:
        np.random.shuffle(indices)
    
    start_idx = 0
    while start_idx &lt; len(indices):
        if start_idx + batch_size &gt; len(indices) - 1:
            excerpt = indices[start_idx:]
        else:
            excerpt = indices[start_idx:start_idx + batch_size]
        
        start_idx += batch_size

        yield inputs[excerpt], targets[excerpt], data[excerpt]</code></pre>
</details>
</dd>
<dt id="PyPruning.ProxPruningClassifier.to_prob_simplex"><code class="name flex">
<span>def <span class="ident">to_prob_simplex</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_prob_simplex(x):
    if x is None or len(x) == 0:
        return x
    sorted_x = np.sort(x)
    x_sum = sorted_x[0]
    l = 1.0 - sorted_x[0]
    for i in range(1,len(sorted_x)):
        x_sum += sorted_x[i]
        tmp = 1.0 / (i + 1.0) * (1.0 - x_sum)
        if (sorted_x[i] + tmp) &gt; 0:
            l = tmp 
    
    return [max(xi + l, 0.0) for xi in x]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PyPruning.ProxPruningClassifier.ProxPruningClassifier"><code class="flex name class">
<span>class <span class="ident">ProxPruningClassifier</span></span>
<span>(</span><span>loss='cross-entropy', step_size=0.1, ensemble_regularizer='L1', l_ensemble_reg=0, tree_regularizer='node', l_tree_reg=0, normalize_weights=True, batch_size=256, epochs=1, verbose=False, update_leaves=False, out_path=None, eval_every_epochs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>(Heterogeneous) Pruning via Proximal Gradient Descent</p>
<p>This pruning method directly minimizes
<span><span class="MathJax_Preview">
\arg\min_w L \left(\sum_{i=1}^M w_i h_i(x), y\right) + \lambda \sum_{i=1}^K w_i R(h_i)
</span><script type="math/tex; mode=display">
\arg\min_w L \left(\sum_{i=1}^M w_i h_i(x), y\right) + \lambda \sum_{i=1}^K w_i R(h_i)
</script></span>
via (stochastic) proximal gradient descent. Currently the <code>{mse, cross-entropy, hinge2}</code> are supported. In addition to the loss functions two types of regularizer can be chosen:</p>
<ul>
<li><code>ensemble_regularizer</code>: This regularizer tries to remove as many members as possible from the ensemble as possible. If you want to select exactly K elements you can choose the <code>hard-L0</code> constraint. Otherwise "soft variations" of this in the form of <code>L0</code> and <code>L1</code> regularization are also available.</li>
<li><code>tree_regularizer</code>: This regularizer tries to choose smaller trees with fewer nodes over larger ones. This regularizer is basically the number of nodes present in a tree.</li>
</ul>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>step_size</code></strong> :&ensp;<code>float</code></dt>
<dd>The step_size used for stochastic gradient descent for opt</dd>
<dt><strong><code>loss</code></strong> :&ensp;<code>str</code></dt>
<dd>The loss function for training. Should be one of <code>{"mse", "cross-entropy", "hinge2"}</code></dd>
<dt><strong><code>normalize_weights</code></strong> :&ensp;<code>boolean</code></dt>
<dd>True if nonzero weights should be projected onto the probability simplex, that is they should sum to 1.</dd>
<dt><strong><code>ensemble_regularizer</code></strong> :&ensp;<code>str</code></dt>
<dd>The ensemble_regularizer. Should be one of <code>{None, "L0", "L1", "hard-L1"}</code></dd>
<dt><strong><code>l_ensemble_reg</code></strong> :&ensp;<code>float</code></dt>
<dd>The ensemble_regularizer regularization strength.</dd>
<dt><strong><code>tree_regularizer</code></strong> :&ensp;<code>str</code></dt>
<dd>The tree_regularizer. Should be one of <code>{None,"node"}</code></dd>
<dt><strong><code>l_tree_reg</code></strong> :&ensp;<code>float</code></dt>
<dd>The tree_regularizer regularization strength.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The batch sized used for SGD</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of epochs SGD is run.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If true, shows a progress bar via tqdm and some statistics</dd>
<dt><strong><code>out_path</code></strong> :&ensp;<code>str</code></dt>
<dd>If set, stores a file called epoch_$i.npy with the statistics for epoch $i under the given path.</dd>
<dt><strong><code>estimators_</code></strong> :&ensp;<code>list</code> of <code>objects</code></dt>
<dd>The list of estimators which are used to built the ensemble. Each estimator must offer a predict_proba method.</dd>
<dt><strong><code>weights_</code></strong> :&ensp;<code>np.array</code> of <code>floats</code></dt>
<dd>The list of weights corresponding to their respective estimator in self.estimators_.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ProxPruningClassifier(PruningClassifier):
    &#34;&#34;&#34; (Heterogeneous) Pruning via Proximal Gradient Descent
    
    This pruning method directly minimizes
    $$
        \\arg\\min_w L \\left(\sum_{i=1}^M w_i h_i(x), y\\right) + \\lambda \\sum_{i=1}^K w_i R(h_i) 
    $$
    via (stochastic) proximal gradient descent. Currently the `{mse, cross-entropy, hinge2}` are supported. In addition to the loss functions two types of regularizer can be chosen:

    - `ensemble_regularizer`: This regularizer tries to remove as many members as possible from the ensemble as possible. If you want to select exactly K elements you can choose the `hard-L0` constraint. Otherwise &#34;soft variations&#34; of this in the form of `L0` and `L1` regularization are also available.
    - `tree_regularizer`: This regularizer tries to choose smaller trees with fewer nodes over larger ones. This regularizer is basically the number of nodes present in a tree.

    Attributes
    ----------
    step_size : float
        The step_size used for stochastic gradient descent for opt 
    loss : str
        The loss function for training. Should be one of `{&#34;mse&#34;, &#34;cross-entropy&#34;, &#34;hinge2&#34;}`
    normalize_weights : boolean
        True if nonzero weights should be projected onto the probability simplex, that is they should sum to 1. 
    ensemble_regularizer : str
        The ensemble_regularizer. Should be one of `{None, &#34;L0&#34;, &#34;L1&#34;, &#34;hard-L1&#34;}`
    l_ensemble_reg : float
        The ensemble_regularizer regularization strength. 
    tree_regularizer : str
        The tree_regularizer. Should be one of `{None,&#34;node&#34;}`
    l_tree_reg : float
        The tree_regularizer regularization strength. 
    batch_size: int
        The batch sized used for SGD
    epochs : int
        The number of epochs SGD is run.
    verbose : boolean
        If true, shows a progress bar via tqdm and some statistics
    out_path: str
        If set, stores a file called epoch_$i.npy with the statistics for epoch $i under the given path.
    estimators_ : list of objects
        The list of estimators which are used to built the ensemble. Each estimator must offer a predict_proba method.
    weights_ : np.array of floats
        The list of weights corresponding to their respective estimator in self.estimators_. 
    &#34;&#34;&#34;

    def __init__(self,
        loss = &#34;cross-entropy&#34;,
        step_size = 1e-1,
        ensemble_regularizer = &#34;L1&#34;,
        l_ensemble_reg = 0,  
        tree_regularizer = &#34;node&#34;,
        l_tree_reg = 0,
        normalize_weights = True,
        batch_size = 256,
        epochs = 1,
        verbose = False, 
        update_leaves = False,
        out_path = None,
        eval_every_epochs = None):

        assert loss in [&#34;mse&#34;,&#34;cross-entropy&#34;,&#34;hinge2&#34;], &#34;Currently only {{mse, cross-entropy, hinge2}} loss is supported&#34;
        assert ensemble_regularizer is None or ensemble_regularizer in [&#34;none&#34;,&#34;L0&#34;, &#34;L1&#34;, &#34;hard-L1&#34;], &#34;Currently only {{none,L0, L1, hard-L1}} the ensemble regularizer is supported&#34;
        assert l_tree_reg &gt;= 0, &#34;l_reg must be greater or equal to 0&#34;
        assert tree_regularizer is None or tree_regularizer in [&#34;node&#34;], &#34;Currently only {{none, node}} regularizer is supported for tree the regularizer.&#34;
        assert batch_size &gt;= 1, &#34;batch_size must be at-least 1&#34;
        assert epochs &gt;= 1, &#34;epochs must be at-least 1&#34;

        if ensemble_regularizer == &#34;hard-L1&#34;:
            assert l_ensemble_reg &gt;= 1 or l_ensemble_reg == 0, &#34;You chose ensemble_regularizer = hard-L1, but set 0 &lt; l_ensemble_reg &lt; 1 which does not really makes sense. If hard-L1 is set, then l_ensemble_reg is the maximum number of estimators in the pruned ensemble, thus likely an integer value &gt;= 1.&#34;

        super().__init__()
        
        self.loss = loss
        self.step_size = step_size
        self.ensemble_regularizer = ensemble_regularizer
        self.l_ensemble_reg = l_ensemble_reg
        self.tree_regularizer = tree_regularizer
        self.l_tree_reg = l_tree_reg
        self.normalize_weights = normalize_weights
        self.batch_size = batch_size
        self.epochs = epochs
        self.verbose = verbose
        self.update_leaves = update_leaves
        self.out_path = out_path
        self.eval_every_epochs = eval_every_epochs

    def next(self, proba, target, data):
        # If we update the leaves, then proba also changes and we need to recompute them. Otherwise we can just use the pre-computed probas
        if self.update_leaves:
            proba = self._individual_proba(data)
        else:
            proba = np.swapaxes(proba, 0, 1)

        output = np.array([w * p for w,p in zip(proba, self.weights_)]).sum(axis=0)

        batch_size = output.shape[0]
        accuracy = (output.argmax(axis=1) == target) * 100.0
        n_trees = [self.num_trees() for _ in range(batch_size)]
        n_param = [self.num_parameters() for _ in range(batch_size)]
        
        # Compute the appropriate loss. 
        if self.loss == &#34;mse&#34;:
            target_one_hot = np.array( [ [1.0 if y == i else 0.0 for i in range(self.n_classes_)] for y in target] )
            loss = (output - target_one_hot) * (output - target_one_hot)
            loss_deriv = 2 * (output - target_one_hot)
        elif self.loss == &#34;cross-entropy&#34;:
            target_one_hot = np.array( [ [1.0 if y == i else 0.0 for i in range(self.n_classes_)] for y in target] )
            p = softmax(output, axis=1)
            loss = -target_one_hot*np.log(p + 1e-7)
            m = target.shape[0]
            loss_deriv = softmax(output, axis=1)
            loss_deriv[range(m),target_one_hot.argmax(axis=1)] -= 1
        elif self.loss == &#34;hinge2&#34;:
            target_one_hot = np.array( [ [1.0 if y == i else -1.0 for i in range(self.n_classes_)] for y in target] )
            zeros = np.zeros_like(target_one_hot)
            loss = np.maximum(1.0 - target_one_hot * output, zeros)**2
            loss_deriv = - 2 * target_one_hot * np.maximum(1.0 - target_one_hot * output, zeros) 
        else:
            raise &#34;Currently only the losses {{cross-entropy, mse, hinge2}} are supported, but you provided: {}&#34;.format(self.loss)
        
        loss = np.sum(np.mean(loss,axis=1))
        
        if self.ensemble_regularizer == &#34;L0&#34;:
            loss += self.l_ensemble_reg * np.linalg.norm(self.weights_,0)
        elif self.ensemble_regularizer == &#34;L1&#34;:
            loss += self.l_ensemble_reg * np.linalg.norm(self.weights_,1)

        # Compute the gradients for the loss
        directions = np.mean(proba*loss_deriv,axis=(1,2))

        # Compute the appropriate regularizer
        if self.tree_regularizer == &#34;node&#34; and self.l_tree_reg &gt; 0:
            loss += self.l_tree_reg * np.sum( [ (w * est.tree_.node_count) for w, est in zip(self.weights_, self.estimators_)] )
            
            node_deriv = self.l_tree_reg * np.array([ est.tree_.node_count for est in self.estimators_])
        else:
            node_deriv = 0

        # Perform the gradient step + projection 
        tmp_w = self.weights_ - self.step_size*directions - self.step_size*node_deriv
        
        if self.update_leaves:
            # compute direction per tree
            # tree_deriv = proba*loss_deriv
            for i, h in enumerate(self.estimators_):
                tree_grad = (self.weights_[i] * loss_deriv)[:,np.newaxis,:]
                # find idx
                idx = h.apply(data)
                h.tree_.value[idx] = h.tree_.value[idx] - self.step_size * tree_grad[:,:,h.classes_.astype(int)]
                # update model
                #h.tree_.value[idx] = h.tree_.value[idx] - self.step_size*h.tree_.value[idx]*tree_deriv[i,:,np.newaxis]
                
                #step = self.step_size*tree_deriv[i,:,np.newaxis]
                #h.tree_.value[idx] = h.tree_.value[idx] - step[:,:,self.classes_.astype(int)]

        if self.ensemble_regularizer == &#34;L0&#34;:
            tmp = np.sqrt(2 * self.l_ensemble_reg * self.step_size)
            tmp_w = np.array([0 if abs(w) &lt; tmp else w for w in tmp_w])
        elif self.ensemble_regularizer == &#34;L1&#34;:
            sign = np.sign(tmp_w)
            tmp_w = np.abs(tmp_w) - self.step_size*self.l_ensemble_reg
            tmp_w = sign*np.maximum(tmp_w,0)
        elif self.ensemble_regularizer == &#34;hard-L1&#34;:
            top_K = np.argsort(tmp_w)[-self.l_ensemble_reg:]
            tmp_w = np.array([w if i in top_K else 0 for i,w in enumerate(tmp_w)])

        # If set, normalize the weights. Note that we use the support of tmp_w for the projection onto the probability simplex
        # as described in http://proceedings.mlr.press/v28/kyrillidis13.pdf
        # Thus, we first need to extract the nonzero weights, project these and then copy them back into corresponding array
        
        if self.normalize_weights and len(tmp_w) &gt; 0:
            nonzero_idx = np.nonzero(tmp_w)[0]
            nonzero_w = tmp_w[nonzero_idx]
            nonzero_w = to_prob_simplex(nonzero_w)
            self.weights_ = np.zeros((len(tmp_w)))
            for i,w in zip(nonzero_idx, nonzero_w):
                self.weights_[i] = w
        else:
            self.weights_ = tmp_w
        
        return {&#34;loss&#34;:loss, &#34;accuracy&#34;: accuracy, &#34;num_trees&#34;: n_trees, &#34;num_parameters&#34; : n_param}

    def num_trees(self):
        return np.count_nonzero(self.weights_)

    def num_parameters(self):
        return sum( [ est.tree_.node_count if w != 0 else 0 for w, est in zip(self.weights_, self.estimators_)] )

    def prune_(self, proba, target, data):
        proba = np.swapaxes(proba, 0, 1)
        self.weights_ = np.array([1.0 / proba.shape[1] for _ in range(proba.shape[1])])

        if self.update_leaves:
            # SKlearn stores the raw counts instead of probabilities. For SGD its better to have the 
            # probabilities for numerical stability. 
            # tree.tree_.value is not writeable, but we can modify the values inplace. Thus we 
            # use [:] to copy the array into the normalized array. Also tree.tree_.value has a strange shape (batch_size, 1, n_classes)
            for tree in self.estimators_:
                tree.tree_.value[:] = tree.tree_.value / tree.tree_.value.sum(axis=(1,2))[:,np.newaxis,np.newaxis]

        for epoch in range(self.epochs):

            mini_batches = create_mini_batches(proba, target, data, self.batch_size, True) 

            times = []
            total_time = 0
            metrics = {}
            example_cnt = 0

            with tqdm(total=proba.shape[0], ncols=150, disable = not self.verbose) as pbar:
                for batch in mini_batches:
                    bproba, btarget, bdata = batch 

                    # Update Model                    
                    start_time = time.time()
                    batch_metrics = self.next(bproba, btarget, bdata)
                    batch_time = time.time() - start_time

                    # Extract statistics
                    for key,val in batch_metrics.items():
                        metrics[key] = np.concatenate( (metrics.get(key,[]), val), axis=None )
                        metrics[key + &#34;_sum&#34;] = metrics.get( key + &#34;_sum&#34;,0) + np.sum(val)

                    example_cnt += bproba.shape[0]
                    pbar.update(bproba.shape[0])
                    
                    # TODO ADD times to metrics and write it to disk
                    times.append(batch_time)
                    total_time += batch_time

                    m_str = &#34;&#34;
                    for key,val in metrics.items():
                        if &#34;_sum&#34; in key:
                            m_str += &#34;{} {:2.4f} &#34;.format(key.split(&#34;_sum&#34;)[0], val / example_cnt)
                    
                    desc = &#39;[{}/{}] {} time_item {:2.4f}&#39;.format(
                        epoch, 
                        self.epochs-1, 
                        m_str,
                        total_time / example_cnt
                    )
                    pbar.set_description(desc)
                
                if self.eval_every_epochs is not None and epoch % self.eval_every_epochs == 0 and self.out_path is not None:
                    np.save(os.path.join(self.out_path, &#34;epoch_{}.npy&#34;.format(epoch)), metrics, allow_pickle=True)
    
        return [i for i in range(len(self.weights_)) if self.weights_[i] &gt; 0], [w for w in self.weights_ if w &gt; 0]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PyPruning.PruningClassifier.PruningClassifier" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier">PruningClassifier</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyPruning.ProxPruningClassifier.ProxPruningClassifier.next"><code class="name flex">
<span>def <span class="ident">next</span></span>(<span>self, proba, target, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def next(self, proba, target, data):
    # If we update the leaves, then proba also changes and we need to recompute them. Otherwise we can just use the pre-computed probas
    if self.update_leaves:
        proba = self._individual_proba(data)
    else:
        proba = np.swapaxes(proba, 0, 1)

    output = np.array([w * p for w,p in zip(proba, self.weights_)]).sum(axis=0)

    batch_size = output.shape[0]
    accuracy = (output.argmax(axis=1) == target) * 100.0
    n_trees = [self.num_trees() for _ in range(batch_size)]
    n_param = [self.num_parameters() for _ in range(batch_size)]
    
    # Compute the appropriate loss. 
    if self.loss == &#34;mse&#34;:
        target_one_hot = np.array( [ [1.0 if y == i else 0.0 for i in range(self.n_classes_)] for y in target] )
        loss = (output - target_one_hot) * (output - target_one_hot)
        loss_deriv = 2 * (output - target_one_hot)
    elif self.loss == &#34;cross-entropy&#34;:
        target_one_hot = np.array( [ [1.0 if y == i else 0.0 for i in range(self.n_classes_)] for y in target] )
        p = softmax(output, axis=1)
        loss = -target_one_hot*np.log(p + 1e-7)
        m = target.shape[0]
        loss_deriv = softmax(output, axis=1)
        loss_deriv[range(m),target_one_hot.argmax(axis=1)] -= 1
    elif self.loss == &#34;hinge2&#34;:
        target_one_hot = np.array( [ [1.0 if y == i else -1.0 for i in range(self.n_classes_)] for y in target] )
        zeros = np.zeros_like(target_one_hot)
        loss = np.maximum(1.0 - target_one_hot * output, zeros)**2
        loss_deriv = - 2 * target_one_hot * np.maximum(1.0 - target_one_hot * output, zeros) 
    else:
        raise &#34;Currently only the losses {{cross-entropy, mse, hinge2}} are supported, but you provided: {}&#34;.format(self.loss)
    
    loss = np.sum(np.mean(loss,axis=1))
    
    if self.ensemble_regularizer == &#34;L0&#34;:
        loss += self.l_ensemble_reg * np.linalg.norm(self.weights_,0)
    elif self.ensemble_regularizer == &#34;L1&#34;:
        loss += self.l_ensemble_reg * np.linalg.norm(self.weights_,1)

    # Compute the gradients for the loss
    directions = np.mean(proba*loss_deriv,axis=(1,2))

    # Compute the appropriate regularizer
    if self.tree_regularizer == &#34;node&#34; and self.l_tree_reg &gt; 0:
        loss += self.l_tree_reg * np.sum( [ (w * est.tree_.node_count) for w, est in zip(self.weights_, self.estimators_)] )
        
        node_deriv = self.l_tree_reg * np.array([ est.tree_.node_count for est in self.estimators_])
    else:
        node_deriv = 0

    # Perform the gradient step + projection 
    tmp_w = self.weights_ - self.step_size*directions - self.step_size*node_deriv
    
    if self.update_leaves:
        # compute direction per tree
        # tree_deriv = proba*loss_deriv
        for i, h in enumerate(self.estimators_):
            tree_grad = (self.weights_[i] * loss_deriv)[:,np.newaxis,:]
            # find idx
            idx = h.apply(data)
            h.tree_.value[idx] = h.tree_.value[idx] - self.step_size * tree_grad[:,:,h.classes_.astype(int)]
            # update model
            #h.tree_.value[idx] = h.tree_.value[idx] - self.step_size*h.tree_.value[idx]*tree_deriv[i,:,np.newaxis]
            
            #step = self.step_size*tree_deriv[i,:,np.newaxis]
            #h.tree_.value[idx] = h.tree_.value[idx] - step[:,:,self.classes_.astype(int)]

    if self.ensemble_regularizer == &#34;L0&#34;:
        tmp = np.sqrt(2 * self.l_ensemble_reg * self.step_size)
        tmp_w = np.array([0 if abs(w) &lt; tmp else w for w in tmp_w])
    elif self.ensemble_regularizer == &#34;L1&#34;:
        sign = np.sign(tmp_w)
        tmp_w = np.abs(tmp_w) - self.step_size*self.l_ensemble_reg
        tmp_w = sign*np.maximum(tmp_w,0)
    elif self.ensemble_regularizer == &#34;hard-L1&#34;:
        top_K = np.argsort(tmp_w)[-self.l_ensemble_reg:]
        tmp_w = np.array([w if i in top_K else 0 for i,w in enumerate(tmp_w)])

    # If set, normalize the weights. Note that we use the support of tmp_w for the projection onto the probability simplex
    # as described in http://proceedings.mlr.press/v28/kyrillidis13.pdf
    # Thus, we first need to extract the nonzero weights, project these and then copy them back into corresponding array
    
    if self.normalize_weights and len(tmp_w) &gt; 0:
        nonzero_idx = np.nonzero(tmp_w)[0]
        nonzero_w = tmp_w[nonzero_idx]
        nonzero_w = to_prob_simplex(nonzero_w)
        self.weights_ = np.zeros((len(tmp_w)))
        for i,w in zip(nonzero_idx, nonzero_w):
            self.weights_[i] = w
    else:
        self.weights_ = tmp_w
    
    return {&#34;loss&#34;:loss, &#34;accuracy&#34;: accuracy, &#34;num_trees&#34;: n_trees, &#34;num_parameters&#34; : n_param}</code></pre>
</details>
</dd>
<dt id="PyPruning.ProxPruningClassifier.ProxPruningClassifier.num_parameters"><code class="name flex">
<span>def <span class="ident">num_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def num_parameters(self):
    return sum( [ est.tree_.node_count if w != 0 else 0 for w, est in zip(self.weights_, self.estimators_)] )</code></pre>
</details>
</dd>
<dt id="PyPruning.ProxPruningClassifier.ProxPruningClassifier.num_trees"><code class="name flex">
<span>def <span class="ident">num_trees</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def num_trees(self):
    return np.count_nonzero(self.weights_)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PyPruning.PruningClassifier.PruningClassifier" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier">PruningClassifier</a></b></code>:
<ul class="hlist">
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.predict" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.predict">predict</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.predict_proba" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.predict_proba">predict_proba</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.prune" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.prune">prune</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.prune_" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.prune_">prune_</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="PyPruning" href="index.html">PyPruning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="PyPruning.ProxPruningClassifier.create_mini_batches" href="#PyPruning.ProxPruningClassifier.create_mini_batches">create_mini_batches</a></code></li>
<li><code><a title="PyPruning.ProxPruningClassifier.to_prob_simplex" href="#PyPruning.ProxPruningClassifier.to_prob_simplex">to_prob_simplex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PyPruning.ProxPruningClassifier.ProxPruningClassifier" href="#PyPruning.ProxPruningClassifier.ProxPruningClassifier">ProxPruningClassifier</a></code></h4>
<ul class="">
<li><code><a title="PyPruning.ProxPruningClassifier.ProxPruningClassifier.next" href="#PyPruning.ProxPruningClassifier.ProxPruningClassifier.next">next</a></code></li>
<li><code><a title="PyPruning.ProxPruningClassifier.ProxPruningClassifier.num_parameters" href="#PyPruning.ProxPruningClassifier.ProxPruningClassifier.num_parameters">num_parameters</a></code></li>
<li><code><a title="PyPruning.ProxPruningClassifier.ProxPruningClassifier.num_trees" href="#PyPruning.ProxPruningClassifier.ProxPruningClassifier.num_trees">num_trees</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
Written by <a href="https://sbuschjaeger.github.io/">Sebastian Buschj√§ger</a> as part of the <a href="https://sfb876.tu-dortmund.de">Collaborative Research Center 876, project A1</a> at the <a href="https://www-ai.cs.tu-dortmund.de">Chair for Artificial Intelligence</a>, TU Dortmund University. Feel free to contact me: <a href="mailto:sebstian.buschjaeger@tu-dortmund.de">sebstian.buschjaeger@tu-dortmund.de</a>.
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>