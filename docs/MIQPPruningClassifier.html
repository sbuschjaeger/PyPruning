<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>PyPruning.MIQPPruningClassifier API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PyPruning.MIQPPruningClassifier</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from functools import partial
import numpy as np
from sklearn import metrics
import cvxpy as cp
from cvxpy import atoms
from joblib import Parallel,delayed
from sklearn.metrics import pairwise

from .PruningClassifier import PruningClassifier

from .RankPruningClassifier import *

def combined(i, j, ensemble_proba, target, weights = [1.0 / 5.0 for _ in range(5)]):
    &#39;&#39;&#39;
    Computes a (weighted) combination of 5 different measures for a pair of classifiers. The original paper also optimizes the weights of this combination using an evolutionary approach and cross-validation. Per default, we use equal weights here. If you want to change this value you can use `partial` to set the weights to different values (e.g. [0.1, 0.1, 0.1, 0.5, 0.2]) before creating a new MIQPPruningClassifier:

    ```Python
        from functools import partial
        m_function = partial(combined, weights = [0.1, 0.1, 0.1, 0.5, 0.2])
        pruner = MIQPPruningClassifier(n_estimators = 10, pairwise_metric = m_function, n_jobs = 8)
    ```

    Reference:
        Cavalcanti, G. D. C., Oliveira, L. S., Moura, T. J. M., &amp; Carvalho, G. V. (2016). Combining diversity measures for ensemble pruning. Pattern Recognition Letters, 74, 38–45. https://doi.org/10.1016/j.patrec.2016.01.029
    &#39;&#39;&#39;
    ipred = ensemble_proba[i,:,:].argmax(axis=1)
    jpred = ensemble_proba[j,:,:].argmax(axis=1)

    a = 0.0   
    b = 0.0   
    c = 0.0   #h1 incorrect, h2 correct
    d = 0.0   #h1 and h2 incorrect
    
    icorr = (ipred == target)
    jcorr = (jpred == target)
    m = len(target)

    #hi and hj correct
    a = np.logical_and(icorr, jcorr).sum()
    
    #hi correct, hj incorrect
    b = np.logical_and(icorr, np.invert(jcorr)).sum()
    
    #hi incorrect, hj correct
    c = np.logical_and(np.invert(icorr), jcorr).sum()

    #hi incorrect, hj incorrect
    d = np.logical_and(np.invert(icorr), np.invert(jcorr)).sum()
    
    # calculate the 5 different metrics
    # 1) disagreement measure 
    dis = (b+c) / m
    
    # 2) qstatistic
    # rare case: divison by zero
    if((a*d) + (b*c) == 0):
        Q = ((a*d) - (b*c)) / 1.0
    else:
        Q = ((a*d) - (b*c)) / ((a*d) + (b*c))
        
    # 3) correlation measure
    # rare case: division by zero
    if((a+b)*(a+c)*(c+d)*(b+d) == 0):
        rho = ((a*d) - (b*c)) / 0.001
    else:
        rho = ((a*d) - (b*c)) / np.sqrt( (a+b)*(a+c)*(c+d)*(b+d) )
        
    # 4) kappa statistic
    kappa1 = (a+d) / m
    kappa2 = ( ((a+b)*(a+c)) + ((c+d)*(b+d)) ) / (m*m)
    
    # rare case: kappa2 == 1
    if(kappa2 == 1):
        kappa2 = kappa2 - 0.001
    kappa = (kappa1 - kappa2) / (1 - kappa2)
    
    # 5) doublefault measure
    df = d / m
    
    # all weighted; disagreement times (-1) so that all metrics are minimized
    return weights[0] * (dis * -1.0) + weights[1] * Q + weights[2] * rho + weights[3] * kappa + weights[4] * df

# # Paper:   Effective pruning of neural network classifier ensembles
# # Authors: Lazarevic et al. 2001
# #
# def disagreement(i, j, ensemble_proba, target):
#     iproba = ensemble_proba[i,:,:].argmax(axis=1)
#     jproba = ensemble_proba[j,:,:].argmax(axis=1)

#     kappa = cohen_kappa_score(iproba, jproba)
    
#     #calculate correlation coefficient
#     m = len(iproba)
#     a = 0   #h1 and h2 correct
#     b = 0   #h1 correct, h2 incorrect
#     c = 0   #h1 incorrect, h2 correct
#     d = 0   #h1 and h2 incorrect
    
#     for j in range(m):
#         if(iproba[j] == target[j] and jproba[j] == target[j]):
#             a = a + 1
#         elif(iproba[j] == target[j] and jproba[j] != target[j]):
#             b = b + 1
#         elif(iproba[j] != target[j] and jproba[j] == target[j]):
#             c = c + 1
#         else:
#             d = d + 1
    
#     # rare case: division by 0
#     if((a+b)*(a+c)*(c+d)*(b+d) == 0):
#         correlation = (a*d) - (b*c) / 0.001
#     else:
#         correlation = ((a*d) - (b*c)) / np.sqrt( (a+b)*(a+c)*(c+d)*(b+d) )
    
#     # equally weighted (no further explanations)
#     return (kappa + correlation) / 2

def combined_error(i, j, ensemble_proba, target):
    &#39;&#39;&#39;
    Computes the pairwise errors of the two classifiers i and j.

    Reference:
        Zhang, Y., Burer, S., &amp; Street, W. N. (2006). Ensemble Pruning Via Semi-definite Programming. Journal of Machine Learning Research, 7, 1315–1338. https://doi.org/10.1016/j.jasms.2006.06.007
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:].argmax(axis=1)
    jproba = ensemble_proba[j,:,:].argmax(axis=1)

    ierr = (iproba != target).astype(np.int32)
    jerr = (jproba != target).astype(np.int32)
    if i == j:
        return (ierr * jerr).mean()
    else:
        Gi = (ierr * ierr).sum()
        Gj = (jerr * jerr).sum()
        combined = ierr * jerr
        return 0.5 * (combined / Gi + combined / Gj).sum()

    # if i == j:
    #     return (ierr*jerr).mean()
    # else:

    # count_h1h2 = 0.0
    # count_h1 = 0.0
    # count_h2 = 0.0
    
    # for j in range(len(iproba)):
    #     if(iproba[j] != target[j]):
    #         count_h1 = count_h1 + 1
    #     if(jproba[j] != target[j]):
    #         count_h2 = count_h2 + 1
    #     if (iproba[j] != target[j] and jproba[j] != target[j]):
    #         count_h1h2 = count_h1h2 + 1
    
    # # avoid rare error: division by 0
    # # if one of these cases occur, count_h1h2 will be 0 aswell
    # # thus count_h1/h2 = 1 wont matter
    # if(count_h1 == 0):
    #     count_h1 = 1.0
    # if(count_h2 == 0):
    #     count_h2 = 1.0
    
    # return ( (count_h1h2 / count_h1 ) + (count_h1h2 / count_h2) ) / 2.0

class MIQPPruningClassifier(PruningClassifier):
    &#39;&#39;&#39; Mixed Integer Quadratic Programming (MIQP) Pruning.

    This pruning method constructs a MIQP so that its solution is the pruned ensemble. Formally, it uses the problem
    
    $$
        \\arg\\min_w (1 - \\alpha ) q^T w + \\alpha w^T P w
    $$

    where \( \\alpha \\in [0,1] \) is the trade-off between the first and the second term. The first vector q contains the individual metrics for each classifier similar to what a RankPruningClassifier would compute, whereas P contains pairwise metrics for each classifier pair in the ensemble. To compute \( q \) and \( P \) there are two metrics required:

    **Single_metric**
    
    This metric assigns a value to each individual classifier in the ensemble without considering pairs of classifier. A single_metric function should accept the following parameters:

    - `i` (int): The classifier which should be rated
    - `ensemble_proba` (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes
    - `target` (list / array): A list / array of class targets.
    
    The single_metric is compatible with the metrics for a RankPruningClassifier. You can use any metric from the RankPruningClassifier here and vice-versa

    **Pairwise_metric**

    This metric assigns a value to each pair of classifiers in the ensemble. A pairwise_metric function should accept the following parameters:

    - `i` (int): The first classifier in the pair
    - `j` (int): The second classifier in the pair 
    - `ensemble_proba` (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes
    - `target` (list / array): A list / array of class targets.
    
    If you set `alpha = 0` or choose the pairwise metric that simply returns 0 a MIQPPruningClassifier should produce the same solution as a RankPruningClassifier does. 

    **Important:** All metrics are _minimized_. If you implement your own metric make sure that it assigns smaller values to better classifiers.
    
    This code uses `cvxpy` to access a wide variety of MQIP solver. For more information on how to configure your solver and interpret its output in case of failures please have a look at the cvxpy documentation https://www.cvxpy.org/tutorial/advanced/index.html#solve-method-options.

    Attributes
    ----------
    n_estimators : int, default is 5
        The number of estimators which should be selected.
    single_metric : function, default is None
        A function that assigns a value to each classifier which forms the q vector
    pairwise_metric : function, default is combined_error
        A function that assigns a value to each pair of classifiers which forms the P matrix
    alpha : float, must be in [0,1]
        The trade-off between the single and pairwise metric. alpha = 0 only considers the single_metric, whereas alpha = 1 only considers the pairwise metric 
    eps : float, default 1e-2
        Sometimes and especially for larger P matrices there can be numerical inaccuries. In this case, the resulting problem might become non-convex so that the MQIP solver cannot solve the problem anymore. For a better numerical stability the eps value can be added to the diagonal of the P matrix. 
    verbose : boolean, default is False
        If true, more information from the MQIP solver is printed. 
    n_jobs : int, default is 8
        The number of threads used for computing the metrics. This does not have any effect on the number of threads used by the MQIP solver.
    &#39;&#39;&#39;

    def __init__(self, n_estimators = 5, single_metric = None, pairwise_metric = combined_error, alpha = 1, eps = 1e-2, verbose = False, n_jobs = 8, **kwargs):
        &#34;&#34;&#34; 
        Creates a new MIQPPruningClassifier.

        Parameters
        ----------

        n_estimators : int, default is 5
            The number of estimators which should be selected.
        single_metric : function, default is None
            A function that assigns a value to each classifier which forms the q vector
        pairwise_metric : function, default is combined_error
            A function that assigns a value to each pair of classifiers which forms the P matrix
        alpha : float, must be in [0,1]
            The trade-off between the single and pairwise metric. alpha = 0 only considers the single_metric, whereas alpha = 1 only considers the pairwise metric 
        eps : float, default 1e-2
            Sometimes and especially for larger P matrices there can be numerical inaccuries. In this case, the resulting problem might become non-convex so that the MQIP solver cannot solve the problem anymore. For a better numerical stability the eps value can be added to the diagonal of the P matrix. 
        verbose : boolean, default is False
            If true, more information from the MQIP solver is printed. 
        n_jobs : int, default is 8
            The number of threads used for computing the metrics. This does not have any effect on the number of threads used by the MQIP solver.
        kwargs : 
            Any additional kwargs are directly supplied to single_metric function and pairwise_metric function via partials
        &#34;&#34;&#34;
        super().__init__()
        
        assert 0 &lt;= alpha &lt;= 1, &#34;l_reg should be from [0,1], but you supplied {}&#34;.format(alpha)
        assert eps &gt;= 0, &#34;Eps should be &gt;= 0, but you supplied&#34;.format(eps)

        assert pairwise_metric is not None or single_metric is not None, &#34;You did not provide a single_metric or pairwise_metric. Please provide at-least one of them&#34;

        if single_metric is None and alpha &lt; 1:
            print(&#34;Warning: You did not provide a single_metric, but set l_reg &lt; 1. This does not make sense. Setting l_reg = 1 for you.&#34;)
            self.alpha = 1

        if pairwise_metric is None and alpha &gt; 0:
            print(&#34;Warning: You did not provide a pairwise_metric, but set l_reg &gt; 0. This does not make sense. Setting l_reg = 0 for you.&#34;)
            self.alpha = 0

        self.n_estimators = n_estimators
        self.n_jobs = n_jobs

        if len(kwargs) &gt; 0:
            self.single_metric = partial(single_metric, **kwargs)
            self.pairwise_metric = partial(pairwise_metric, **kwargs)
        else:    
            self.single_metric = single_metric
            self.pairwise_metric = pairwise_metric
        
        self.alpha = alpha
        self.verbose = verbose
        self.eps = eps

    def prune_(self, proba, target, data = None):
        n_received = len(proba)
        if self.n_estimators &gt;= n_received:
            return range(0, n_received), [1.0 / n_received for _ in range(n_received)]

        if self.alpha &lt; 1:
            single_scores = Parallel(n_jobs=self.n_jobs, backend=&#34;threading&#34;)(
                delayed(self.single_metric) (i, proba, target) for i in range(n_received)
            )
            q = np.array(single_scores)
        else:
            q = np.zeros((n_received,1))

        if self.alpha &gt; 0:
            pairwise_scores = Parallel(n_jobs=self.n_jobs, backend=&#34;threading&#34;)(
                delayed(self.pairwise_metric) (i, j, proba, target) for i in range(n_received) for j in range(i, n_received)
            )

            # TODO This is probably easier and quicker with some fancy numpy operations
            P = np.zeros((n_received,n_received))
            s = 0
            for i in range(n_received):
                for j in range(i, n_received):
                    if i == j:
                        P[i,j] = pairwise_scores[s]
                    else:
                        P[i,j] = pairwise_scores[s]
                        P[j,i] = pairwise_scores[s]
                    s += 1
            P += self.eps * np.eye(n_received)

        else:
            P = np.zeros((n_received,n_received))

        w = cp.Variable(n_received, boolean=True)
        
        if self.alpha == 1:
            objective = cp.quad_form(w, P) 
        elif self.alpha == 0:
            objective = q.T @ w
        else:
            objective = cp.pos((1.0 - self.alpha)) * q.T @ w + cp.pos(self.alpha) * cp.quad_form(w, P)

        prob = cp.Problem(cp.Minimize(objective), [
            atoms.affine.sum.sum(w) == self.n_estimators,
        ]) 
        prob.solve(verbose=self.verbose)
        selected = [i for i in range(n_received) if w.value[i]]
        weights = [1.0/len(selected) for _ in selected]

        return selected, weights</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="PyPruning.MIQPPruningClassifier.combined"><code class="name flex">
<span>def <span class="ident">combined</span></span>(<span>i, j, ensemble_proba, target, weights=[0.2, 0.2, 0.2, 0.2, 0.2])</span>
</code></dt>
<dd>
<div class="desc"><p>Computes a (weighted) combination of 5 different measures for a pair of classifiers. The original paper also optimizes the weights of this combination using an evolutionary approach and cross-validation. Per default, we use equal weights here. If you want to change this value you can use <code>partial</code> to set the weights to different values (e.g. [0.1, 0.1, 0.1, 0.5, 0.2]) before creating a new MIQPPruningClassifier:</p>
<pre><code class="language-Python">    from functools import partial
    m_function = partial(combined, weights = [0.1, 0.1, 0.1, 0.5, 0.2])
    pruner = MIQPPruningClassifier(n_estimators = 10, pairwise_metric = m_function, n_jobs = 8)
</code></pre>
<h2 id="reference">Reference</h2>
<p>Cavalcanti, G. D. C., Oliveira, L. S., Moura, T. J. M., &amp; Carvalho, G. V. (2016). Combining diversity measures for ensemble pruning. Pattern Recognition Letters, 74, 38–45. <a href="https://doi.org/10.1016/j.patrec.2016.01.029">https://doi.org/10.1016/j.patrec.2016.01.029</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combined(i, j, ensemble_proba, target, weights = [1.0 / 5.0 for _ in range(5)]):
    &#39;&#39;&#39;
    Computes a (weighted) combination of 5 different measures for a pair of classifiers. The original paper also optimizes the weights of this combination using an evolutionary approach and cross-validation. Per default, we use equal weights here. If you want to change this value you can use `partial` to set the weights to different values (e.g. [0.1, 0.1, 0.1, 0.5, 0.2]) before creating a new MIQPPruningClassifier:

    ```Python
        from functools import partial
        m_function = partial(combined, weights = [0.1, 0.1, 0.1, 0.5, 0.2])
        pruner = MIQPPruningClassifier(n_estimators = 10, pairwise_metric = m_function, n_jobs = 8)
    ```

    Reference:
        Cavalcanti, G. D. C., Oliveira, L. S., Moura, T. J. M., &amp; Carvalho, G. V. (2016). Combining diversity measures for ensemble pruning. Pattern Recognition Letters, 74, 38–45. https://doi.org/10.1016/j.patrec.2016.01.029
    &#39;&#39;&#39;
    ipred = ensemble_proba[i,:,:].argmax(axis=1)
    jpred = ensemble_proba[j,:,:].argmax(axis=1)

    a = 0.0   
    b = 0.0   
    c = 0.0   #h1 incorrect, h2 correct
    d = 0.0   #h1 and h2 incorrect
    
    icorr = (ipred == target)
    jcorr = (jpred == target)
    m = len(target)

    #hi and hj correct
    a = np.logical_and(icorr, jcorr).sum()
    
    #hi correct, hj incorrect
    b = np.logical_and(icorr, np.invert(jcorr)).sum()
    
    #hi incorrect, hj correct
    c = np.logical_and(np.invert(icorr), jcorr).sum()

    #hi incorrect, hj incorrect
    d = np.logical_and(np.invert(icorr), np.invert(jcorr)).sum()
    
    # calculate the 5 different metrics
    # 1) disagreement measure 
    dis = (b+c) / m
    
    # 2) qstatistic
    # rare case: divison by zero
    if((a*d) + (b*c) == 0):
        Q = ((a*d) - (b*c)) / 1.0
    else:
        Q = ((a*d) - (b*c)) / ((a*d) + (b*c))
        
    # 3) correlation measure
    # rare case: division by zero
    if((a+b)*(a+c)*(c+d)*(b+d) == 0):
        rho = ((a*d) - (b*c)) / 0.001
    else:
        rho = ((a*d) - (b*c)) / np.sqrt( (a+b)*(a+c)*(c+d)*(b+d) )
        
    # 4) kappa statistic
    kappa1 = (a+d) / m
    kappa2 = ( ((a+b)*(a+c)) + ((c+d)*(b+d)) ) / (m*m)
    
    # rare case: kappa2 == 1
    if(kappa2 == 1):
        kappa2 = kappa2 - 0.001
    kappa = (kappa1 - kappa2) / (1 - kappa2)
    
    # 5) doublefault measure
    df = d / m
    
    # all weighted; disagreement times (-1) so that all metrics are minimized
    return weights[0] * (dis * -1.0) + weights[1] * Q + weights[2] * rho + weights[3] * kappa + weights[4] * df</code></pre>
</details>
</dd>
<dt id="PyPruning.MIQPPruningClassifier.combined_error"><code class="name flex">
<span>def <span class="ident">combined_error</span></span>(<span>i, j, ensemble_proba, target)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the pairwise errors of the two classifiers i and j.</p>
<h2 id="reference">Reference</h2>
<p>Zhang, Y., Burer, S., &amp; Street, W. N. (2006). Ensemble Pruning Via Semi-definite Programming. Journal of Machine Learning Research, 7, 1315–1338. <a href="https://doi.org/10.1016/j.jasms.2006.06.007">https://doi.org/10.1016/j.jasms.2006.06.007</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combined_error(i, j, ensemble_proba, target):
    &#39;&#39;&#39;
    Computes the pairwise errors of the two classifiers i and j.

    Reference:
        Zhang, Y., Burer, S., &amp; Street, W. N. (2006). Ensemble Pruning Via Semi-definite Programming. Journal of Machine Learning Research, 7, 1315–1338. https://doi.org/10.1016/j.jasms.2006.06.007
    &#39;&#39;&#39;
    iproba = ensemble_proba[i,:,:].argmax(axis=1)
    jproba = ensemble_proba[j,:,:].argmax(axis=1)

    ierr = (iproba != target).astype(np.int32)
    jerr = (jproba != target).astype(np.int32)
    if i == j:
        return (ierr * jerr).mean()
    else:
        Gi = (ierr * ierr).sum()
        Gj = (jerr * jerr).sum()
        combined = ierr * jerr
        return 0.5 * (combined / Gi + combined / Gj).sum()

    # if i == j:
    #     return (ierr*jerr).mean()
    # else:

    # count_h1h2 = 0.0
    # count_h1 = 0.0
    # count_h2 = 0.0
    
    # for j in range(len(iproba)):
    #     if(iproba[j] != target[j]):
    #         count_h1 = count_h1 + 1
    #     if(jproba[j] != target[j]):
    #         count_h2 = count_h2 + 1
    #     if (iproba[j] != target[j] and jproba[j] != target[j]):
    #         count_h1h2 = count_h1h2 + 1
    
    # # avoid rare error: division by 0
    # # if one of these cases occur, count_h1h2 will be 0 aswell
    # # thus count_h1/h2 = 1 wont matter
    # if(count_h1 == 0):
    #     count_h1 = 1.0
    # if(count_h2 == 0):
    #     count_h2 = 1.0
    
    # return ( (count_h1h2 / count_h1 ) + (count_h1h2 / count_h2) ) / 2.0</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PyPruning.MIQPPruningClassifier.MIQPPruningClassifier"><code class="flex name class">
<span>class <span class="ident">MIQPPruningClassifier</span></span>
<span>(</span><span>n_estimators=5, single_metric=None, pairwise_metric=&lt;function combined_error&gt;, alpha=1, eps=0.01, verbose=False, n_jobs=8, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Mixed Integer Quadratic Programming (MIQP) Pruning.</p>
<p>This pruning method constructs a MIQP so that its solution is the pruned ensemble. Formally, it uses the problem</p>
<p><span><span class="MathJax_Preview">
\arg\min_w (1 - \alpha ) q^T w + \alpha w^T P w
</span><script type="math/tex; mode=display">
\arg\min_w (1 - \alpha ) q^T w + \alpha w^T P w
</script></span></p>
<p>where <span><span class="MathJax_Preview"> \alpha \in [0,1] </span><script type="math/tex"> \alpha \in [0,1] </script></span> is the trade-off between the first and the second term. The first vector q contains the individual metrics for each classifier similar to what a RankPruningClassifier would compute, whereas P contains pairwise metrics for each classifier pair in the ensemble. To compute <span><span class="MathJax_Preview"> q </span><script type="math/tex"> q </script></span> and <span><span class="MathJax_Preview"> P </span><script type="math/tex"> P </script></span> there are two metrics required:</p>
<p><strong>Single_metric</strong></p>
<p>This metric assigns a value to each individual classifier in the ensemble without considering pairs of classifier. A single_metric function should accept the following parameters:</p>
<ul>
<li><code>i</code> (int): The classifier which should be rated</li>
<li><code>ensemble_proba</code> (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes</li>
<li><code>target</code> (list / array): A list / array of class targets.</li>
</ul>
<p>The single_metric is compatible with the metrics for a RankPruningClassifier. You can use any metric from the RankPruningClassifier here and vice-versa</p>
<p><strong>Pairwise_metric</strong></p>
<p>This metric assigns a value to each pair of classifiers in the ensemble. A pairwise_metric function should accept the following parameters:</p>
<ul>
<li><code>i</code> (int): The first classifier in the pair</li>
<li><code>j</code> (int): The second classifier in the pair </li>
<li><code>ensemble_proba</code> (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes</li>
<li><code>target</code> (list / array): A list / array of class targets.</li>
</ul>
<p>If you set <code>alpha = 0</code> or choose the pairwise metric that simply returns 0 a MIQPPruningClassifier should produce the same solution as a RankPruningClassifier does. </p>
<p><strong>Important:</strong> All metrics are <em>minimized</em>. If you implement your own metric make sure that it assigns smaller values to better classifiers.</p>
<p>This code uses <code>cvxpy</code> to access a wide variety of MQIP solver. For more information on how to configure your solver and interpret its output in case of failures please have a look at the cvxpy documentation <a href="https://www.cvxpy.org/tutorial/advanced/index.html#solve-method-options.">https://www.cvxpy.org/tutorial/advanced/index.html#solve-method-options.</a></p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>n_estimators</code></strong> :&ensp;<code>int</code>, default <code>is 5</code></dt>
<dd>The number of estimators which should be selected.</dd>
<dt><strong><code>single_metric</code></strong> :&ensp;<code>function</code>, default <code>is None</code></dt>
<dd>A function that assigns a value to each classifier which forms the q vector</dd>
<dt><strong><code>pairwise_metric</code></strong> :&ensp;<code>function</code>, default <code>is <a title="PyPruning.MIQPPruningClassifier.combined_error" href="#PyPruning.MIQPPruningClassifier.combined_error">combined_error()</a></code></dt>
<dd>A function that assigns a value to each pair of classifiers which forms the P matrix</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float, must be in [0,1]</code></dt>
<dd>The trade-off between the single and pairwise metric. alpha = 0 only considers the single_metric, whereas alpha = 1 only considers the pairwise metric</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, default <code>1e-2</code></dt>
<dd>Sometimes and especially for larger P matrices there can be numerical inaccuries. In this case, the resulting problem might become non-convex so that the MQIP solver cannot solve the problem anymore. For a better numerical stability the eps value can be added to the diagonal of the P matrix.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boolean</code>, default <code>is False</code></dt>
<dd>If true, more information from the MQIP solver is printed.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, default <code>is 8</code></dt>
<dd>The number of threads used for computing the metrics. This does not have any effect on the number of threads used by the MQIP solver.</dd>
</dl>
<p>Creates a new MIQPPruningClassifier.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_estimators</code></strong> :&ensp;<code>int</code>, default <code>is 5</code></dt>
<dd>The number of estimators which should be selected.</dd>
<dt><strong><code>single_metric</code></strong> :&ensp;<code>function</code>, default <code>is None</code></dt>
<dd>A function that assigns a value to each classifier which forms the q vector</dd>
<dt><strong><code>pairwise_metric</code></strong> :&ensp;<code>function</code>, default <code>is <a title="PyPruning.MIQPPruningClassifier.combined_error" href="#PyPruning.MIQPPruningClassifier.combined_error">combined_error()</a></code></dt>
<dd>A function that assigns a value to each pair of classifiers which forms the P matrix</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float, must be in [0,1]</code></dt>
<dd>The trade-off between the single and pairwise metric. alpha = 0 only considers the single_metric, whereas alpha = 1 only considers the pairwise metric</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, default <code>1e-2</code></dt>
<dd>Sometimes and especially for larger P matrices there can be numerical inaccuries. In this case, the resulting problem might become non-convex so that the MQIP solver cannot solve the problem anymore. For a better numerical stability the eps value can be added to the diagonal of the P matrix.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boolean</code>, default <code>is False</code></dt>
<dd>If true, more information from the MQIP solver is printed.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, default <code>is 8</code></dt>
<dd>The number of threads used for computing the metrics. This does not have any effect on the number of threads used by the MQIP solver.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Any additional kwargs are directly supplied to single_metric function and pairwise_metric function via partials</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MIQPPruningClassifier(PruningClassifier):
    &#39;&#39;&#39; Mixed Integer Quadratic Programming (MIQP) Pruning.

    This pruning method constructs a MIQP so that its solution is the pruned ensemble. Formally, it uses the problem
    
    $$
        \\arg\\min_w (1 - \\alpha ) q^T w + \\alpha w^T P w
    $$

    where \( \\alpha \\in [0,1] \) is the trade-off between the first and the second term. The first vector q contains the individual metrics for each classifier similar to what a RankPruningClassifier would compute, whereas P contains pairwise metrics for each classifier pair in the ensemble. To compute \( q \) and \( P \) there are two metrics required:

    **Single_metric**
    
    This metric assigns a value to each individual classifier in the ensemble without considering pairs of classifier. A single_metric function should accept the following parameters:

    - `i` (int): The classifier which should be rated
    - `ensemble_proba` (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes
    - `target` (list / array): A list / array of class targets.
    
    The single_metric is compatible with the metrics for a RankPruningClassifier. You can use any metric from the RankPruningClassifier here and vice-versa

    **Pairwise_metric**

    This metric assigns a value to each pair of classifiers in the ensemble. A pairwise_metric function should accept the following parameters:

    - `i` (int): The first classifier in the pair
    - `j` (int): The second classifier in the pair 
    - `ensemble_proba` (A (M, N, C) matrix ): All N predictions of all M classifier in the entire ensemble for all C classes
    - `target` (list / array): A list / array of class targets.
    
    If you set `alpha = 0` or choose the pairwise metric that simply returns 0 a MIQPPruningClassifier should produce the same solution as a RankPruningClassifier does. 

    **Important:** All metrics are _minimized_. If you implement your own metric make sure that it assigns smaller values to better classifiers.
    
    This code uses `cvxpy` to access a wide variety of MQIP solver. For more information on how to configure your solver and interpret its output in case of failures please have a look at the cvxpy documentation https://www.cvxpy.org/tutorial/advanced/index.html#solve-method-options.

    Attributes
    ----------
    n_estimators : int, default is 5
        The number of estimators which should be selected.
    single_metric : function, default is None
        A function that assigns a value to each classifier which forms the q vector
    pairwise_metric : function, default is combined_error
        A function that assigns a value to each pair of classifiers which forms the P matrix
    alpha : float, must be in [0,1]
        The trade-off between the single and pairwise metric. alpha = 0 only considers the single_metric, whereas alpha = 1 only considers the pairwise metric 
    eps : float, default 1e-2
        Sometimes and especially for larger P matrices there can be numerical inaccuries. In this case, the resulting problem might become non-convex so that the MQIP solver cannot solve the problem anymore. For a better numerical stability the eps value can be added to the diagonal of the P matrix. 
    verbose : boolean, default is False
        If true, more information from the MQIP solver is printed. 
    n_jobs : int, default is 8
        The number of threads used for computing the metrics. This does not have any effect on the number of threads used by the MQIP solver.
    &#39;&#39;&#39;

    def __init__(self, n_estimators = 5, single_metric = None, pairwise_metric = combined_error, alpha = 1, eps = 1e-2, verbose = False, n_jobs = 8, **kwargs):
        &#34;&#34;&#34; 
        Creates a new MIQPPruningClassifier.

        Parameters
        ----------

        n_estimators : int, default is 5
            The number of estimators which should be selected.
        single_metric : function, default is None
            A function that assigns a value to each classifier which forms the q vector
        pairwise_metric : function, default is combined_error
            A function that assigns a value to each pair of classifiers which forms the P matrix
        alpha : float, must be in [0,1]
            The trade-off between the single and pairwise metric. alpha = 0 only considers the single_metric, whereas alpha = 1 only considers the pairwise metric 
        eps : float, default 1e-2
            Sometimes and especially for larger P matrices there can be numerical inaccuries. In this case, the resulting problem might become non-convex so that the MQIP solver cannot solve the problem anymore. For a better numerical stability the eps value can be added to the diagonal of the P matrix. 
        verbose : boolean, default is False
            If true, more information from the MQIP solver is printed. 
        n_jobs : int, default is 8
            The number of threads used for computing the metrics. This does not have any effect on the number of threads used by the MQIP solver.
        kwargs : 
            Any additional kwargs are directly supplied to single_metric function and pairwise_metric function via partials
        &#34;&#34;&#34;
        super().__init__()
        
        assert 0 &lt;= alpha &lt;= 1, &#34;l_reg should be from [0,1], but you supplied {}&#34;.format(alpha)
        assert eps &gt;= 0, &#34;Eps should be &gt;= 0, but you supplied&#34;.format(eps)

        assert pairwise_metric is not None or single_metric is not None, &#34;You did not provide a single_metric or pairwise_metric. Please provide at-least one of them&#34;

        if single_metric is None and alpha &lt; 1:
            print(&#34;Warning: You did not provide a single_metric, but set l_reg &lt; 1. This does not make sense. Setting l_reg = 1 for you.&#34;)
            self.alpha = 1

        if pairwise_metric is None and alpha &gt; 0:
            print(&#34;Warning: You did not provide a pairwise_metric, but set l_reg &gt; 0. This does not make sense. Setting l_reg = 0 for you.&#34;)
            self.alpha = 0

        self.n_estimators = n_estimators
        self.n_jobs = n_jobs

        if len(kwargs) &gt; 0:
            self.single_metric = partial(single_metric, **kwargs)
            self.pairwise_metric = partial(pairwise_metric, **kwargs)
        else:    
            self.single_metric = single_metric
            self.pairwise_metric = pairwise_metric
        
        self.alpha = alpha
        self.verbose = verbose
        self.eps = eps

    def prune_(self, proba, target, data = None):
        n_received = len(proba)
        if self.n_estimators &gt;= n_received:
            return range(0, n_received), [1.0 / n_received for _ in range(n_received)]

        if self.alpha &lt; 1:
            single_scores = Parallel(n_jobs=self.n_jobs, backend=&#34;threading&#34;)(
                delayed(self.single_metric) (i, proba, target) for i in range(n_received)
            )
            q = np.array(single_scores)
        else:
            q = np.zeros((n_received,1))

        if self.alpha &gt; 0:
            pairwise_scores = Parallel(n_jobs=self.n_jobs, backend=&#34;threading&#34;)(
                delayed(self.pairwise_metric) (i, j, proba, target) for i in range(n_received) for j in range(i, n_received)
            )

            # TODO This is probably easier and quicker with some fancy numpy operations
            P = np.zeros((n_received,n_received))
            s = 0
            for i in range(n_received):
                for j in range(i, n_received):
                    if i == j:
                        P[i,j] = pairwise_scores[s]
                    else:
                        P[i,j] = pairwise_scores[s]
                        P[j,i] = pairwise_scores[s]
                    s += 1
            P += self.eps * np.eye(n_received)

        else:
            P = np.zeros((n_received,n_received))

        w = cp.Variable(n_received, boolean=True)
        
        if self.alpha == 1:
            objective = cp.quad_form(w, P) 
        elif self.alpha == 0:
            objective = q.T @ w
        else:
            objective = cp.pos((1.0 - self.alpha)) * q.T @ w + cp.pos(self.alpha) * cp.quad_form(w, P)

        prob = cp.Problem(cp.Minimize(objective), [
            atoms.affine.sum.sum(w) == self.n_estimators,
        ]) 
        prob.solve(verbose=self.verbose)
        selected = [i for i in range(n_received) if w.value[i]]
        weights = [1.0/len(selected) for _ in selected]

        return selected, weights</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="PyPruning.PruningClassifier.PruningClassifier" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier">PruningClassifier</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="PyPruning.PruningClassifier.PruningClassifier" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier">PruningClassifier</a></b></code>:
<ul class="hlist">
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.predict" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.predict">predict</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.predict_proba" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.predict_proba">predict_proba</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.prune" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.prune">prune</a></code></li>
<li><code><a title="PyPruning.PruningClassifier.PruningClassifier.prune_" href="PruningClassifier.html#PyPruning.PruningClassifier.PruningClassifier.prune_">prune_</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="PyPruning" href="index.html">PyPruning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="PyPruning.MIQPPruningClassifier.combined" href="#PyPruning.MIQPPruningClassifier.combined">combined</a></code></li>
<li><code><a title="PyPruning.MIQPPruningClassifier.combined_error" href="#PyPruning.MIQPPruningClassifier.combined_error">combined_error</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PyPruning.MIQPPruningClassifier.MIQPPruningClassifier" href="#PyPruning.MIQPPruningClassifier.MIQPPruningClassifier">MIQPPruningClassifier</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
Written by <a href="https://sbuschjaeger.github.io/">Sebastian Buschjäger</a> as part of the <a href="https://sfb876.tu-dortmund.de">Collaborative Research Center 876, project A1</a> at the <a href="https://www-ai.cs.tu-dortmund.de">Chair for Artificial Intelligence</a>, TU Dortmund University. Feel free to contact me: <a href="mailto:sebstian.buschjaeger@tu-dortmund.de">sebstian.buschjaeger@tu-dortmund.de</a>.
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>